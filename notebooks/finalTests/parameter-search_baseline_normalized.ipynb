{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter search with optuna - BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-09T08:10:02.375818Z",
     "end_time": "2023-12-09T08:10:07.399812Z"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "from etnn import TreeNode\n",
    "from etnn.nn.layer_framework import LayerManagementFramework\n",
    "from etnn.routines.run_config import choice_dataset, choice_trainloader, choice_loss, choice_optim\n",
    "from etnn.tools.training import train_epoch, eval_epoch\n",
    "from etnn.tools.training_tools import ConfigStore, seeding_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of objective function for ETNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-09T08:10:07.396812Z",
     "end_time": "2023-12-09T08:10:07.418320Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # init default config\n",
    "    config = ConfigStore(\n",
    "        in_dim=15,\n",
    "        hidden_dim=0, #trial.suggest_int(\"hidden_dim\", 16, 512, step=16),\n",
    "        out_dim=1,\n",
    "        k=0, #trial.suggest_int(\"k\", 1, 5),\n",
    "        dataset=-1,\n",
    "        ds_size=10_000,\n",
    "        num_gondolas=10,\n",
    "        num_part_pg=5,\n",
    "        loss_name='mse',\n",
    "        optimizer_name='adam',\n",
    "        num_max_epochs=30, # real: 100\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True),\n",
    "        batch_size=1024,\n",
    "        early_stop_tol=5,\n",
    "        use_equal_batcher=trial.suggest_categorical(\"batcher\", [True, False]),\n",
    "        seed=420,\n",
    "        label_type=label,\n",
    "        final_label_factor=1/1000\n",
    "    )\n",
    "\n",
    "    # loading dataset\n",
    "    dataset, df_index = choice_dataset(config, dataset_path)\n",
    "    # splitting off test dataset\n",
    "    generator = torch.Generator().manual_seed(config.seed)\n",
    "    train_ds, val_ds, _ = random_split(\n",
    "        dataset,\n",
    "        [1 - test_perc - val_perc, val_perc, test_perc],\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "    # loaders\n",
    "    train_loader = choice_trainloader(config, df_index, train_ds)\n",
    "    val_loader = DataLoader(val_ds, batch_size=4 * config.batch_size, shuffle=False)\n",
    "\n",
    "    # build tree\n",
    "    tree_structure = TreeNode(\n",
    "        node_type=\"C\",\n",
    "        children=[\n",
    "            TreeNode(\"P\", [TreeNode(\"E\", config.num_part_pg)])\n",
    "            for _ in range(config.num_gondolas)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # define device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # set seed for reproducability\n",
    "    seeding_all(config.seed)\n",
    "\n",
    "    # define model\n",
    "    layer_list = [torch.nn.Flatten()]\n",
    "    features = config.in_dim * config.num_gondolas * config.num_part_pg\n",
    "\n",
    "    # for each layer create a linear layer and relu (except last one)\n",
    "    for i in range(trial.suggest_int(\"n_layers\", 1, 5)-1):\n",
    "        # determine new feature dimension\n",
    "        new_features = trial.suggest_int(f\"n_dim_{i}\", 1, 512)\n",
    "\n",
    "        # add layer and relu to list\n",
    "        layer_list += [torch.nn.Linear(features, new_features), torch.nn.ReLU()]\n",
    "\n",
    "        # set the new feature to be the current feature\n",
    "        features = new_features\n",
    "\n",
    "    # set the last layer - this one must map to the out dimension\n",
    "    layer_list += [torch.nn.Linear(features, config.out_dim)]\n",
    "    model = torch.nn.Sequential(*layer_list).to(device)\n",
    "\n",
    "    # learning tools\n",
    "    criterion = choice_loss(config)\n",
    "    optimizer = choice_optim(config, model)\n",
    "\n",
    "    # init score list\n",
    "    score_list = []\n",
    "\n",
    "    # train for specified number of epochs\n",
    "    for epoch in range(config.num_max_epochs):\n",
    "        _, _, _ = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            device,\n",
    "            criterion\n",
    "        )\n",
    "\n",
    "        _, val_true_y, val_pred_y = eval_epoch(\n",
    "            model,\n",
    "            val_loader,\n",
    "            device,\n",
    "            criterion\n",
    "        )\n",
    "\n",
    "        # calc r2 score and append\n",
    "        score = r2_score(y_true=val_true_y, y_pred=val_pred_y)\n",
    "        score_list += [score]\n",
    "        trial.report(score, epoch)\n",
    "\n",
    "    # calculate objective\n",
    "    # display(score_list)\n",
    "    # idea: last x r2 scores (why not last one? for stability purposes)\n",
    "    obj = np.array(score_list)[-stability_count:]\n",
    "    return np.mean(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree advanced label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T19:56:49.719229Z",
     "end_time": "2023-12-08T19:56:49.738248Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting global parameters\n",
    "dataset_path = \"../../datasets/\"\n",
    "label = \"tree_advanced\" # alt: tree or default\n",
    "test_perc = 0.3\n",
    "val_perc = 0.21\n",
    "stability_count = 5\n",
    "n_trials = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T19:56:49.734747Z",
     "end_time": "2023-12-08T21:20:08.701550Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 19:56:49,734] A new study created in memory with name: Best tree advanced label config\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/70 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ccaf819cb0e49e1ae2aee4e06ed078e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 19:57:58,943] Trial 0 finished with value: -939.4449011030545 and parameters: {'learning_rate': 2.4006605088264315e-05, 'batcher': False, 'n_layers': 1}. Best is trial 0 with value: -939.4449011030545.\n",
      "[I 2023-12-08 19:59:19,023] Trial 1 finished with value: 0.9201144923544472 and parameters: {'learning_rate': 0.002104284711699982, 'batcher': False, 'n_layers': 4, 'n_dim_0': 219, 'n_dim_1': 401, 'n_dim_2': 355}. Best is trial 1 with value: 0.9201144923544472.\n",
      "[I 2023-12-08 20:00:40,962] Trial 2 finished with value: -1004.7290627621171 and parameters: {'learning_rate': 3.064636911462103e-05, 'batcher': False, 'n_layers': 5, 'n_dim_0': 89, 'n_dim_1': 304, 'n_dim_2': 315, 'n_dim_3': 1}. Best is trial 1 with value: 0.9201144923544472.\n",
      "[I 2023-12-08 20:02:05,508] Trial 3 finished with value: 0.8962070188960141 and parameters: {'learning_rate': 0.00029349149611142326, 'batcher': False, 'n_layers': 2, 'n_dim_0': 208}. Best is trial 1 with value: 0.9201144923544472.\n",
      "[I 2023-12-08 20:03:23,213] Trial 4 finished with value: 0.9055881370849903 and parameters: {'learning_rate': 0.0007282096147110321, 'batcher': True, 'n_layers': 2, 'n_dim_0': 250}. Best is trial 1 with value: 0.9201144923544472.\n",
      "[I 2023-12-08 20:04:44,712] Trial 5 finished with value: -993.624634550896 and parameters: {'learning_rate': 1.1311738324281564e-05, 'batcher': False, 'n_layers': 4, 'n_dim_0': 290, 'n_dim_1': 4, 'n_dim_2': 124}. Best is trial 1 with value: 0.9201144923544472.\n",
      "[I 2023-12-08 20:06:09,097] Trial 6 finished with value: -465.6051316576466 and parameters: {'learning_rate': 4.089666520723408e-05, 'batcher': False, 'n_layers': 4, 'n_dim_0': 10, 'n_dim_1': 306, 'n_dim_2': 481}. Best is trial 1 with value: 0.9201144923544472.\n",
      "[I 2023-12-08 20:07:29,723] Trial 7 finished with value: 0.8485639115971821 and parameters: {'learning_rate': 0.000131484746754946, 'batcher': False, 'n_layers': 3, 'n_dim_0': 234, 'n_dim_1': 113}. Best is trial 1 with value: 0.9201144923544472.\n",
      "[I 2023-12-08 20:08:49,208] Trial 8 finished with value: 0.6371897439994275 and parameters: {'learning_rate': 0.00010105382081448794, 'batcher': False, 'n_layers': 5, 'n_dim_0': 21, 'n_dim_1': 408, 'n_dim_2': 106, 'n_dim_3': 321}. Best is trial 1 with value: 0.9201144923544472.\n",
      "[I 2023-12-08 20:10:12,048] Trial 9 finished with value: 0.9298934646097419 and parameters: {'learning_rate': 0.0021640628309100973, 'batcher': False, 'n_layers': 3, 'n_dim_0': 300, 'n_dim_1': 170}. Best is trial 9 with value: 0.9298934646097419.\n",
      "[I 2023-12-08 20:11:25,546] Trial 10 finished with value: 0.939308391074331 and parameters: {'learning_rate': 0.006810065904661181, 'batcher': True, 'n_layers': 3, 'n_dim_0': 492, 'n_dim_1': 151}. Best is trial 10 with value: 0.939308391074331.\n",
      "[I 2023-12-08 20:12:43,790] Trial 11 finished with value: 0.9126776964599028 and parameters: {'learning_rate': 0.007335143865807707, 'batcher': True, 'n_layers': 3, 'n_dim_0': 512, 'n_dim_1': 144}. Best is trial 10 with value: 0.939308391074331.\n",
      "[I 2023-12-08 20:14:07,974] Trial 12 finished with value: 0.9258175994473129 and parameters: {'learning_rate': 0.008634350702532705, 'batcher': True, 'n_layers': 2, 'n_dim_0': 501}. Best is trial 10 with value: 0.939308391074331.\n",
      "[I 2023-12-08 20:15:29,367] Trial 13 finished with value: 0.9339033673525897 and parameters: {'learning_rate': 0.002305902761577434, 'batcher': True, 'n_layers': 3, 'n_dim_0': 376, 'n_dim_1': 173}. Best is trial 10 with value: 0.939308391074331.\n",
      "[I 2023-12-08 20:16:52,627] Trial 14 finished with value: 0.8371002559445596 and parameters: {'learning_rate': 0.003248121941145798, 'batcher': True, 'n_layers': 1}. Best is trial 10 with value: 0.939308391074331.\n",
      "[I 2023-12-08 20:18:17,919] Trial 15 finished with value: 0.9014414043379565 and parameters: {'learning_rate': 0.0008565111539130362, 'batcher': True, 'n_layers': 2, 'n_dim_0': 405}. Best is trial 10 with value: 0.939308391074331.\n",
      "[I 2023-12-08 20:19:46,620] Trial 16 finished with value: 0.933645010618085 and parameters: {'learning_rate': 0.004376516836814639, 'batcher': True, 'n_layers': 4, 'n_dim_0': 405, 'n_dim_1': 202, 'n_dim_2': 7}. Best is trial 10 with value: 0.939308391074331.\n",
      "[I 2023-12-08 20:21:13,501] Trial 17 finished with value: 0.9579897192771221 and parameters: {'learning_rate': 0.00949924946536999, 'batcher': True, 'n_layers': 3, 'n_dim_0': 408, 'n_dim_1': 49}. Best is trial 17 with value: 0.9579897192771221.\n",
      "[I 2023-12-08 20:22:37,896] Trial 18 finished with value: 0.9407069163061085 and parameters: {'learning_rate': 0.009645617348184962, 'batcher': True, 'n_layers': 3, 'n_dim_0': 454, 'n_dim_1': 28}. Best is trial 17 with value: 0.9579897192771221.\n",
      "[I 2023-12-08 20:23:44,097] Trial 19 finished with value: 0.9644312023521735 and parameters: {'learning_rate': 0.008466422572167911, 'batcher': True, 'n_layers': 4, 'n_dim_0': 353, 'n_dim_1': 7, 'n_dim_2': 484}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:24:48,313] Trial 20 finished with value: 0.913752896172982 and parameters: {'learning_rate': 0.001352962385907345, 'batcher': True, 'n_layers': 5, 'n_dim_0': 342, 'n_dim_1': 70, 'n_dim_2': 487, 'n_dim_3': 483}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:25:53,679] Trial 21 finished with value: 0.9260372462564831 and parameters: {'learning_rate': 0.009084596244391166, 'batcher': True, 'n_layers': 4, 'n_dim_0': 442, 'n_dim_1': 8, 'n_dim_2': 387}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:26:58,311] Trial 22 finished with value: 0.9493676934382214 and parameters: {'learning_rate': 0.004264048908737209, 'batcher': True, 'n_layers': 4, 'n_dim_0': 433, 'n_dim_1': 69, 'n_dim_2': 242}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:28:03,292] Trial 23 finished with value: 0.9350138597220065 and parameters: {'learning_rate': 0.003939209439550552, 'batcher': True, 'n_layers': 4, 'n_dim_0': 345, 'n_dim_1': 77, 'n_dim_2': 226}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:29:08,187] Trial 24 finished with value: 0.9292304199889626 and parameters: {'learning_rate': 0.004936838880993335, 'batcher': True, 'n_layers': 5, 'n_dim_0': 433, 'n_dim_1': 72, 'n_dim_2': 231, 'n_dim_3': 19}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:30:11,228] Trial 25 finished with value: 0.9386613848508476 and parameters: {'learning_rate': 0.003694798812389439, 'batcher': True, 'n_layers': 4, 'n_dim_0': 339, 'n_dim_1': 223, 'n_dim_2': 164}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:31:13,541] Trial 26 finished with value: 0.9209501390116273 and parameters: {'learning_rate': 0.005855660600003501, 'batcher': True, 'n_layers': 4, 'n_dim_0': 394, 'n_dim_1': 68, 'n_dim_2': 422}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:32:29,134] Trial 27 finished with value: 0.8903487036898821 and parameters: {'learning_rate': 0.0011312910329665349, 'batcher': True, 'n_layers': 5, 'n_dim_0': 162, 'n_dim_1': 492, 'n_dim_2': 301, 'n_dim_3': 219}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:33:38,233] Trial 28 finished with value: 0.9158249102204771 and parameters: {'learning_rate': 0.005649250885136129, 'batcher': True, 'n_layers': 4, 'n_dim_0': 293, 'n_dim_1': 101, 'n_dim_2': 272}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:34:38,756] Trial 29 finished with value: 0.8632748630248426 and parameters: {'learning_rate': 0.009727147668432579, 'batcher': True, 'n_layers': 1}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:35:41,142] Trial 30 finished with value: 0.9269484892311001 and parameters: {'learning_rate': 0.0028696278560047324, 'batcher': True, 'n_layers': 3, 'n_dim_0': 460, 'n_dim_1': 33}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:36:43,185] Trial 31 finished with value: 0.9471680201084693 and parameters: {'learning_rate': 0.009744117606332267, 'batcher': True, 'n_layers': 3, 'n_dim_0': 452, 'n_dim_1': 29}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:37:44,497] Trial 32 finished with value: 0.936504448817457 and parameters: {'learning_rate': 0.005258790065671598, 'batcher': True, 'n_layers': 3, 'n_dim_0': 374, 'n_dim_1': 42}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:38:46,274] Trial 33 finished with value: 0.9316339644760653 and parameters: {'learning_rate': 0.0030601912282606967, 'batcher': True, 'n_layers': 3, 'n_dim_0': 475, 'n_dim_1': 113}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:39:48,320] Trial 34 finished with value: 0.918931861048647 and parameters: {'learning_rate': 0.0016960436985858178, 'batcher': True, 'n_layers': 2, 'n_dim_0': 413}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:40:50,873] Trial 35 finished with value: 0.933303415558765 and parameters: {'learning_rate': 0.006197729785092861, 'batcher': True, 'n_layers': 4, 'n_dim_0': 359, 'n_dim_1': 6, 'n_dim_2': 184}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:42:05,276] Trial 36 finished with value: 0.9510954839661515 and parameters: {'learning_rate': 0.009750329810546953, 'batcher': True, 'n_layers': 2, 'n_dim_0': 433}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:43:07,448] Trial 37 finished with value: 0.9329203790126386 and parameters: {'learning_rate': 0.004306233412929154, 'batcher': True, 'n_layers': 2, 'n_dim_0': 314}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:44:09,837] Trial 38 finished with value: 0.8367455839560185 and parameters: {'learning_rate': 0.0024149644799878163, 'batcher': False, 'n_layers': 1}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:45:12,110] Trial 39 finished with value: 0.8903655082640007 and parameters: {'learning_rate': 0.00048282096525936014, 'batcher': True, 'n_layers': 2, 'n_dim_0': 196}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:46:16,276] Trial 40 finished with value: 0.9223327712212246 and parameters: {'learning_rate': 0.0018330274495553574, 'batcher': False, 'n_layers': 5, 'n_dim_0': 267, 'n_dim_1': 277, 'n_dim_2': 61, 'n_dim_3': 481}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:47:21,256] Trial 41 finished with value: 0.9225520905232324 and parameters: {'learning_rate': 0.009983098192843832, 'batcher': True, 'n_layers': 3, 'n_dim_0': 431, 'n_dim_1': 49}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:48:25,490] Trial 42 finished with value: 0.9432786198089282 and parameters: {'learning_rate': 0.006690925968317369, 'batcher': True, 'n_layers': 2, 'n_dim_0': 467}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:49:29,475] Trial 43 finished with value: 0.9449682497355081 and parameters: {'learning_rate': 0.006817193921378128, 'batcher': True, 'n_layers': 3, 'n_dim_0': 427, 'n_dim_1': 105}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:50:33,489] Trial 44 finished with value: 0.9273426187161439 and parameters: {'learning_rate': 0.004751945407738815, 'batcher': True, 'n_layers': 4, 'n_dim_0': 386, 'n_dim_1': 36, 'n_dim_2': 450}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:51:42,705] Trial 45 finished with value: 0.9463485814839651 and parameters: {'learning_rate': 0.006804174894193472, 'batcher': False, 'n_layers': 3, 'n_dim_0': 478, 'n_dim_1': 2}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:53:01,048] Trial 46 finished with value: 0.922930934762723 and parameters: {'learning_rate': 0.0035434556911161223, 'batcher': True, 'n_layers': 4, 'n_dim_0': 318, 'n_dim_1': 128, 'n_dim_2': 357}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:54:04,716] Trial 47 finished with value: 0.9531950401763737 and parameters: {'learning_rate': 0.007341340042405144, 'batcher': True, 'n_layers': 3, 'n_dim_0': 421, 'n_dim_1': 83}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:55:17,437] Trial 48 finished with value: 0.9277897327529987 and parameters: {'learning_rate': 0.002501628247537146, 'batcher': False, 'n_layers': 2, 'n_dim_0': 373}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:56:23,173] Trial 49 finished with value: 0.9182492560245666 and parameters: {'learning_rate': 0.007366392845944377, 'batcher': True, 'n_layers': 4, 'n_dim_0': 64, 'n_dim_1': 86, 'n_dim_2': 190}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:57:26,527] Trial 50 finished with value: 0.8431844899871933 and parameters: {'learning_rate': 0.004717900374040663, 'batcher': True, 'n_layers': 1}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:58:31,482] Trial 51 finished with value: 0.9515136437270121 and parameters: {'learning_rate': 0.007815579563335564, 'batcher': True, 'n_layers': 3, 'n_dim_0': 426, 'n_dim_1': 54}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 20:59:36,995] Trial 52 finished with value: 0.9555405188504622 and parameters: {'learning_rate': 0.008014696688191457, 'batcher': True, 'n_layers': 3, 'n_dim_0': 414, 'n_dim_1': 58}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:00:42,203] Trial 53 finished with value: 0.9522257159835877 and parameters: {'learning_rate': 0.007916508559622823, 'batcher': True, 'n_layers': 3, 'n_dim_0': 412, 'n_dim_1': 339}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:01:46,784] Trial 54 finished with value: 0.9076583233057225 and parameters: {'learning_rate': 0.00707129359545054, 'batcher': True, 'n_layers': 3, 'n_dim_0': 414, 'n_dim_1': 342}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:02:49,385] Trial 55 finished with value: 0.9419107095566295 and parameters: {'learning_rate': 0.0032327663425095288, 'batcher': True, 'n_layers': 3, 'n_dim_0': 397, 'n_dim_1': 361}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:04:03,868] Trial 56 finished with value: 0.9263613426464536 and parameters: {'learning_rate': 0.008176769148566139, 'batcher': True, 'n_layers': 3, 'n_dim_0': 488, 'n_dim_1': 394}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:05:23,252] Trial 57 finished with value: 0.9372328392377993 and parameters: {'learning_rate': 0.005520320767696093, 'batcher': True, 'n_layers': 3, 'n_dim_0': 361, 'n_dim_1': 463}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:06:43,300] Trial 58 finished with value: 0.9325296908836649 and parameters: {'learning_rate': 0.004003670206439472, 'batcher': True, 'n_layers': 3, 'n_dim_0': 411, 'n_dim_1': 308}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:07:59,605] Trial 59 finished with value: 0.9228185735630141 and parameters: {'learning_rate': 0.008264514954164463, 'batcher': False, 'n_layers': 3, 'n_dim_0': 326, 'n_dim_1': 233}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:09:09,805] Trial 60 finished with value: 0.9537553843514635 and parameters: {'learning_rate': 0.005243961094356972, 'batcher': True, 'n_layers': 3, 'n_dim_0': 512, 'n_dim_1': 178}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:10:21,379] Trial 61 finished with value: 0.9254871203957284 and parameters: {'learning_rate': 0.006096213441445701, 'batcher': True, 'n_layers': 3, 'n_dim_0': 505, 'n_dim_1': 177}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:11:35,220] Trial 62 finished with value: 0.910466308837964 and parameters: {'learning_rate': 0.007704307967957096, 'batcher': True, 'n_layers': 3, 'n_dim_0': 456, 'n_dim_1': 129}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:12:47,922] Trial 63 finished with value: 0.9339642385703808 and parameters: {'learning_rate': 0.0050640533381528935, 'batcher': True, 'n_layers': 3, 'n_dim_0': 396, 'n_dim_1': 52}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:14:01,702] Trial 64 finished with value: 0.9120379622668737 and parameters: {'learning_rate': 0.008125407956984911, 'batcher': True, 'n_layers': 3, 'n_dim_0': 512, 'n_dim_1': 87}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:15:14,487] Trial 65 finished with value: 0.9397059948982877 and parameters: {'learning_rate': 0.003786625404393997, 'batcher': True, 'n_layers': 3, 'n_dim_0': 360, 'n_dim_1': 153}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:16:23,830] Trial 66 finished with value: 0.9449775465605048 and parameters: {'learning_rate': 0.005395549051098949, 'batcher': True, 'n_layers': 3, 'n_dim_0': 446, 'n_dim_1': 56}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:17:36,604] Trial 67 finished with value: 0.9315768994598578 and parameters: {'learning_rate': 0.007961295281468127, 'batcher': True, 'n_layers': 3, 'n_dim_0': 485, 'n_dim_1': 19}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:18:54,545] Trial 68 finished with value: 0.9233583532922527 and parameters: {'learning_rate': 0.005930743866591937, 'batcher': True, 'n_layers': 3, 'n_dim_0': 387, 'n_dim_1': 262}. Best is trial 19 with value: 0.9644312023521735.\n",
      "[I 2023-12-08 21:20:08,694] Trial 69 finished with value: 0.9342570207689084 and parameters: {'learning_rate': 0.0029441143553212415, 'batcher': True, 'n_layers': 3, 'n_dim_0': 423, 'n_dim_1': 203}. Best is trial 19 with value: 0.9644312023521735.\n"
     ]
    }
   ],
   "source": [
    "study_tree_advanced = optuna.create_study(study_name=\"Best tree advanced label config\", directions=['maximize'])\n",
    "study_tree_advanced.optimize(objective, n_trials=n_trials, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T21:20:08.703530Z",
     "end_time": "2023-12-08T21:20:08.718529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.008466422572167911, 'batcher': True, 'n_layers': 4, 'n_dim_0': 353, 'n_dim_1': 7, 'n_dim_2': 484}\n"
     ]
    }
   ],
   "source": [
    "best_par_tree_advanced = study_tree_advanced.best_params\n",
    "print(best_par_tree_advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T21:20:08.718529Z",
     "end_time": "2023-12-08T21:20:08.744530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'TPESampler'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_tree_advanced.sampler.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T21:20:08.735529Z",
     "end_time": "2023-12-08T21:20:08.771531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    number        value             datetime_start          datetime_complete  \\\n0        0  -939.444901 2023-12-08 19:56:49.744254 2023-12-08 19:57:58.943975   \n1        1     0.920114 2023-12-08 19:57:58.945480 2023-12-08 19:59:19.023901   \n2        2 -1004.729063 2023-12-08 19:59:19.025902 2023-12-08 20:00:40.961248   \n3        3     0.896207 2023-12-08 20:00:40.963248 2023-12-08 20:02:05.507228   \n4        4     0.905588 2023-12-08 20:02:05.509229 2023-12-08 20:03:23.213526   \n..     ...          ...                        ...                        ...   \n65      65     0.939706 2023-12-08 21:14:01.704534 2023-12-08 21:15:14.487481   \n66      66     0.944978 2023-12-08 21:15:14.488480 2023-12-08 21:16:23.830428   \n67      67     0.931577 2023-12-08 21:16:23.832429 2023-12-08 21:17:36.604296   \n68      68     0.923358 2023-12-08 21:17:36.605310 2023-12-08 21:18:54.544129   \n69      69     0.934257 2023-12-08 21:18:54.546128 2023-12-08 21:20:08.693019   \n\n                 duration  params_batcher  params_learning_rate  \\\n0  0 days 00:01:09.199721           False              0.000024   \n1  0 days 00:01:20.078421           False              0.002104   \n2  0 days 00:01:21.935346           False              0.000031   \n3  0 days 00:01:24.543980           False              0.000293   \n4  0 days 00:01:17.704297            True              0.000728   \n..                    ...             ...                   ...   \n65 0 days 00:01:12.782947            True              0.003787   \n66 0 days 00:01:09.341948            True              0.005396   \n67 0 days 00:01:12.771867            True              0.007961   \n68 0 days 00:01:17.938819            True              0.005931   \n69 0 days 00:01:14.146891            True              0.002944   \n\n    params_n_dim_0  params_n_dim_1  params_n_dim_2  params_n_dim_3  \\\n0              NaN             NaN             NaN             NaN   \n1            219.0           401.0           355.0             NaN   \n2             89.0           304.0           315.0             1.0   \n3            208.0             NaN             NaN             NaN   \n4            250.0             NaN             NaN             NaN   \n..             ...             ...             ...             ...   \n65           360.0           153.0             NaN             NaN   \n66           446.0            56.0             NaN             NaN   \n67           485.0            19.0             NaN             NaN   \n68           387.0           262.0             NaN             NaN   \n69           423.0           203.0             NaN             NaN   \n\n    params_n_layers     state  \n0                 1  COMPLETE  \n1                 4  COMPLETE  \n2                 5  COMPLETE  \n3                 2  COMPLETE  \n4                 2  COMPLETE  \n..              ...       ...  \n65                3  COMPLETE  \n66                3  COMPLETE  \n67                3  COMPLETE  \n68                3  COMPLETE  \n69                3  COMPLETE  \n\n[70 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number</th>\n      <th>value</th>\n      <th>datetime_start</th>\n      <th>datetime_complete</th>\n      <th>duration</th>\n      <th>params_batcher</th>\n      <th>params_learning_rate</th>\n      <th>params_n_dim_0</th>\n      <th>params_n_dim_1</th>\n      <th>params_n_dim_2</th>\n      <th>params_n_dim_3</th>\n      <th>params_n_layers</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-939.444901</td>\n      <td>2023-12-08 19:56:49.744254</td>\n      <td>2023-12-08 19:57:58.943975</td>\n      <td>0 days 00:01:09.199721</td>\n      <td>False</td>\n      <td>0.000024</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.920114</td>\n      <td>2023-12-08 19:57:58.945480</td>\n      <td>2023-12-08 19:59:19.023901</td>\n      <td>0 days 00:01:20.078421</td>\n      <td>False</td>\n      <td>0.002104</td>\n      <td>219.0</td>\n      <td>401.0</td>\n      <td>355.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-1004.729063</td>\n      <td>2023-12-08 19:59:19.025902</td>\n      <td>2023-12-08 20:00:40.961248</td>\n      <td>0 days 00:01:21.935346</td>\n      <td>False</td>\n      <td>0.000031</td>\n      <td>89.0</td>\n      <td>304.0</td>\n      <td>315.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.896207</td>\n      <td>2023-12-08 20:00:40.963248</td>\n      <td>2023-12-08 20:02:05.507228</td>\n      <td>0 days 00:01:24.543980</td>\n      <td>False</td>\n      <td>0.000293</td>\n      <td>208.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.905588</td>\n      <td>2023-12-08 20:02:05.509229</td>\n      <td>2023-12-08 20:03:23.213526</td>\n      <td>0 days 00:01:17.704297</td>\n      <td>True</td>\n      <td>0.000728</td>\n      <td>250.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>65</td>\n      <td>0.939706</td>\n      <td>2023-12-08 21:14:01.704534</td>\n      <td>2023-12-08 21:15:14.487481</td>\n      <td>0 days 00:01:12.782947</td>\n      <td>True</td>\n      <td>0.003787</td>\n      <td>360.0</td>\n      <td>153.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>66</td>\n      <td>0.944978</td>\n      <td>2023-12-08 21:15:14.488480</td>\n      <td>2023-12-08 21:16:23.830428</td>\n      <td>0 days 00:01:09.341948</td>\n      <td>True</td>\n      <td>0.005396</td>\n      <td>446.0</td>\n      <td>56.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>67</td>\n      <td>0.931577</td>\n      <td>2023-12-08 21:16:23.832429</td>\n      <td>2023-12-08 21:17:36.604296</td>\n      <td>0 days 00:01:12.771867</td>\n      <td>True</td>\n      <td>0.007961</td>\n      <td>485.0</td>\n      <td>19.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>68</td>\n      <td>0.923358</td>\n      <td>2023-12-08 21:17:36.605310</td>\n      <td>2023-12-08 21:18:54.544129</td>\n      <td>0 days 00:01:17.938819</td>\n      <td>True</td>\n      <td>0.005931</td>\n      <td>387.0</td>\n      <td>262.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>69</td>\n      <td>0.934257</td>\n      <td>2023-12-08 21:18:54.546128</td>\n      <td>2023-12-08 21:20:08.693019</td>\n      <td>0 days 00:01:14.146891</td>\n      <td>True</td>\n      <td>0.002944</td>\n      <td>423.0</td>\n      <td>203.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n  </tbody>\n</table>\n<p>70 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tree_advanced = study_tree_advanced.trials_dataframe()\n",
    "df_tree_advanced.to_csv(\"study_label-tree-advanced_baseline_normalized.csv\")\n",
    "display(df_tree_advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T21:20:08.766529Z",
     "end_time": "2023-12-08T21:20:08.782530Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting global parameters\n",
    "dataset_path = \"../../datasets/\"\n",
    "label = \"tree\" # alt: tree or default\n",
    "test_perc = 0.3\n",
    "val_perc = 0.21\n",
    "stability_count = 5\n",
    "n_trials = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T21:20:08.783536Z",
     "end_time": "2023-12-08T22:22:45.605945Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 21:20:08,783] A new study created in memory with name: Best tree label config\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f291eec091e141ff88950bb42f3cdf72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-12-08 21:21:18,473] Trial 0 finished with value: 0.8660064817618178 and parameters: {'learning_rate': 0.008646976968890727, 'batcher': True, 'n_layers': 4, 'n_dim_0': 181, 'n_dim_1': 132, 'n_dim_2': 125}. Best is trial 0 with value: 0.8660064817618178.\n",
      "[I 2023-12-08 21:22:29,376] Trial 1 finished with value: 0.7972039692698816 and parameters: {'learning_rate': 0.0006975641348274792, 'batcher': False, 'n_layers': 2, 'n_dim_0': 190}. Best is trial 0 with value: 0.8660064817618178.\n",
      "[I 2023-12-08 21:23:45,153] Trial 2 finished with value: 0.8130472632062355 and parameters: {'learning_rate': 0.004318543004301319, 'batcher': False, 'n_layers': 4, 'n_dim_0': 403, 'n_dim_1': 324, 'n_dim_2': 495}. Best is trial 0 with value: 0.8660064817618178.\n",
      "[I 2023-12-08 21:25:01,064] Trial 3 finished with value: 0.8875302762419887 and parameters: {'learning_rate': 0.0065806054377971785, 'batcher': False, 'n_layers': 4, 'n_dim_0': 274, 'n_dim_1': 11, 'n_dim_2': 427}. Best is trial 3 with value: 0.8875302762419887.\n",
      "[I 2023-12-08 21:26:13,470] Trial 4 finished with value: 0.7653639896593778 and parameters: {'learning_rate': 0.00017938501619613588, 'batcher': False, 'n_layers': 2, 'n_dim_0': 441}. Best is trial 3 with value: 0.8875302762419887.\n",
      "[I 2023-12-08 21:27:22,298] Trial 5 finished with value: -749.3154666033421 and parameters: {'learning_rate': 4.275881027213853e-05, 'batcher': True, 'n_layers': 1}. Best is trial 3 with value: 0.8875302762419887.\n",
      "[I 2023-12-08 21:28:35,023] Trial 6 finished with value: -245.4494115070927 and parameters: {'learning_rate': 2.2964017509466786e-05, 'batcher': False, 'n_layers': 5, 'n_dim_0': 219, 'n_dim_1': 420, 'n_dim_2': 163, 'n_dim_3': 378}. Best is trial 3 with value: 0.8875302762419887.\n",
      "[I 2023-12-08 21:29:48,098] Trial 7 finished with value: -643.6817541292614 and parameters: {'learning_rate': 7.163439557395948e-05, 'batcher': True, 'n_layers': 1}. Best is trial 3 with value: 0.8875302762419887.\n",
      "[I 2023-12-08 21:31:05,536] Trial 8 finished with value: -740.8464017342378 and parameters: {'learning_rate': 1.1455493247549336e-05, 'batcher': True, 'n_layers': 3, 'n_dim_0': 494, 'n_dim_1': 42}. Best is trial 3 with value: 0.8875302762419887.\n",
      "[I 2023-12-08 21:32:21,574] Trial 9 finished with value: 0.7498971271412808 and parameters: {'learning_rate': 0.00020010964855159027, 'batcher': False, 'n_layers': 5, 'n_dim_0': 257, 'n_dim_1': 185, 'n_dim_2': 423, 'n_dim_3': 352}. Best is trial 3 with value: 0.8875302762419887.\n",
      "[I 2023-12-08 21:33:29,973] Trial 10 finished with value: -13.468859057212592 and parameters: {'learning_rate': 0.001378000053196894, 'batcher': False, 'n_layers': 4, 'n_dim_0': 15, 'n_dim_1': 1, 'n_dim_2': 331}. Best is trial 3 with value: 0.8875302762419887.\n",
      "[I 2023-12-08 21:34:43,950] Trial 11 finished with value: 0.8376388853319415 and parameters: {'learning_rate': 0.009355943529528161, 'batcher': True, 'n_layers': 4, 'n_dim_0': 111, 'n_dim_1': 145, 'n_dim_2': 33}. Best is trial 3 with value: 0.8875302762419887.\n",
      "[I 2023-12-08 21:35:55,972] Trial 12 finished with value: 0.9316672875257372 and parameters: {'learning_rate': 0.009597949948171597, 'batcher': True, 'n_layers': 3, 'n_dim_0': 330, 'n_dim_1': 134}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:37:12,130] Trial 13 finished with value: 0.8512660454236375 and parameters: {'learning_rate': 0.0025503169112210387, 'batcher': True, 'n_layers': 3, 'n_dim_0': 334, 'n_dim_1': 85}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:38:25,001] Trial 14 finished with value: 0.8544525231214634 and parameters: {'learning_rate': 0.0028246605406092094, 'batcher': True, 'n_layers': 2, 'n_dim_0': 308}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:39:33,694] Trial 15 finished with value: 0.8215245567879768 and parameters: {'learning_rate': 0.0008924376014309771, 'batcher': False, 'n_layers': 3, 'n_dim_0': 361, 'n_dim_1': 255}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:40:46,712] Trial 16 finished with value: -0.02952984674332195 and parameters: {'learning_rate': 0.009498797951972634, 'batcher': False, 'n_layers': 5, 'n_dim_0': 298, 'n_dim_1': 227, 'n_dim_2': 314, 'n_dim_3': 5}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:42:04,906] Trial 17 finished with value: 0.8463153515891646 and parameters: {'learning_rate': 0.004230360330164057, 'batcher': True, 'n_layers': 4, 'n_dim_0': 118, 'n_dim_1': 85, 'n_dim_2': 510}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:43:25,486] Trial 18 finished with value: 0.7986807445107313 and parameters: {'learning_rate': 0.0004539642557904489, 'batcher': True, 'n_layers': 3, 'n_dim_0': 377, 'n_dim_1': 10}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:44:40,632] Trial 19 finished with value: 0.8442258988124319 and parameters: {'learning_rate': 0.001914696048546327, 'batcher': False, 'n_layers': 2, 'n_dim_0': 271}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:45:53,613] Trial 20 finished with value: 0.8247846543333246 and parameters: {'learning_rate': 0.004302872439388049, 'batcher': False, 'n_layers': 4, 'n_dim_0': 457, 'n_dim_1': 319, 'n_dim_2': 365}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:47:08,808] Trial 21 finished with value: 0.9107669221514028 and parameters: {'learning_rate': 0.008958742348806602, 'batcher': True, 'n_layers': 3, 'n_dim_0': 175, 'n_dim_1': 142}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:48:26,826] Trial 22 finished with value: 0.8698265197969896 and parameters: {'learning_rate': 0.006455324674279224, 'batcher': True, 'n_layers': 3, 'n_dim_0': 154, 'n_dim_1': 122}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:49:43,662] Trial 23 finished with value: 0.8875355773585124 and parameters: {'learning_rate': 0.004485402332447493, 'batcher': True, 'n_layers': 3, 'n_dim_0': 216, 'n_dim_1': 193}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:50:55,312] Trial 24 finished with value: 0.8013505255000701 and parameters: {'learning_rate': 0.0015341093999875028, 'batcher': True, 'n_layers': 2, 'n_dim_0': 76}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:52:06,602] Trial 25 finished with value: 0.8641225065736036 and parameters: {'learning_rate': 0.0033734055091471033, 'batcher': True, 'n_layers': 3, 'n_dim_0': 224, 'n_dim_1': 192}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:53:21,133] Trial 26 finished with value: 0.9214525780247683 and parameters: {'learning_rate': 0.005500655120765626, 'batcher': True, 'n_layers': 3, 'n_dim_0': 228, 'n_dim_1': 299}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:54:38,921] Trial 27 finished with value: 0.8187360385530351 and parameters: {'learning_rate': 0.009711086154697434, 'batcher': True, 'n_layers': 3, 'n_dim_0': 150, 'n_dim_1': 309}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:55:53,216] Trial 28 finished with value: 0.805996393021905 and parameters: {'learning_rate': 0.0022320721805015775, 'batcher': True, 'n_layers': 2, 'n_dim_0': 66}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:57:02,905] Trial 29 finished with value: 0.7272776169979844 and parameters: {'learning_rate': 0.006366991604464243, 'batcher': True, 'n_layers': 3, 'n_dim_0': 325, 'n_dim_1': 437}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:58:09,876] Trial 30 finished with value: 0.8778308077072545 and parameters: {'learning_rate': 0.006128885066374874, 'batcher': True, 'n_layers': 2, 'n_dim_0': 238}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 21:59:21,568] Trial 31 finished with value: 0.8671148351925633 and parameters: {'learning_rate': 0.004192536512273394, 'batcher': True, 'n_layers': 3, 'n_dim_0': 202, 'n_dim_1': 214}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:00:38,178] Trial 32 finished with value: 0.8731955328211012 and parameters: {'learning_rate': 0.00588180883173212, 'batcher': True, 'n_layers': 3, 'n_dim_0': 159, 'n_dim_1': 279}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:01:56,233] Trial 33 finished with value: 0.8558483827933575 and parameters: {'learning_rate': 0.0027800419350980086, 'batcher': True, 'n_layers': 3, 'n_dim_0': 187, 'n_dim_1': 362}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:03:07,607] Trial 34 finished with value: -0.4022025013211592 and parameters: {'learning_rate': 0.009354669600719083, 'batcher': True, 'n_layers': 4, 'n_dim_0': 252, 'n_dim_1': 512, 'n_dim_2': 211}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:04:20,411] Trial 35 finished with value: 0.8636711806113617 and parameters: {'learning_rate': 0.0043115938724538755, 'batcher': True, 'n_layers': 2, 'n_dim_0': 184}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:05:38,502] Trial 36 finished with value: 0.8666020983688792 and parameters: {'learning_rate': 0.006279216202454316, 'batcher': True, 'n_layers': 4, 'n_dim_0': 280, 'n_dim_1': 153, 'n_dim_2': 8}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:07:00,850] Trial 37 finished with value: 0.8105286473700758 and parameters: {'learning_rate': 0.001126985634811343, 'batcher': True, 'n_layers': 3, 'n_dim_0': 354, 'n_dim_1': 87}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:08:15,189] Trial 38 finished with value: 0.8399701542829696 and parameters: {'learning_rate': 0.0018242558543701017, 'batcher': True, 'n_layers': 2, 'n_dim_0': 399}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:09:27,022] Trial 39 finished with value: 0.714965527821289 and parameters: {'learning_rate': 0.0032095274047959192, 'batcher': True, 'n_layers': 1}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:10:44,378] Trial 40 finished with value: 0.7948044003186773 and parameters: {'learning_rate': 0.0007347180589119973, 'batcher': True, 'n_layers': 3, 'n_dim_0': 118, 'n_dim_1': 174}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:12:04,288] Trial 41 finished with value: 0.8259883900199199 and parameters: {'learning_rate': 0.0067015717431284485, 'batcher': False, 'n_layers': 4, 'n_dim_0': 213, 'n_dim_1': 233, 'n_dim_2': 397}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:13:15,338] Trial 42 finished with value: 0.9037227561567516 and parameters: {'learning_rate': 0.005309755466557232, 'batcher': False, 'n_layers': 4, 'n_dim_0': 278, 'n_dim_1': 50, 'n_dim_2': 270}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:14:31,523] Trial 43 finished with value: 0.8914420673097968 and parameters: {'learning_rate': 0.004431201938464726, 'batcher': False, 'n_layers': 4, 'n_dim_0': 294, 'n_dim_1': 117, 'n_dim_2': 258}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:15:45,127] Trial 44 finished with value: 0.8928757011042885 and parameters: {'learning_rate': 0.00732762054529174, 'batcher': False, 'n_layers': 5, 'n_dim_0': 302, 'n_dim_1': 116, 'n_dim_2': 263, 'n_dim_3': 90}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:17:02,027] Trial 45 finished with value: -0.0027699545269123773 and parameters: {'learning_rate': 0.009969177659117843, 'batcher': False, 'n_layers': 5, 'n_dim_0': 326, 'n_dim_1': 67, 'n_dim_2': 257, 'n_dim_3': 81}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:18:21,891] Trial 46 finished with value: 0.8129870627893844 and parameters: {'learning_rate': 0.007524371964446023, 'batcher': False, 'n_layers': 5, 'n_dim_0': 256, 'n_dim_1': 47, 'n_dim_2': 106, 'n_dim_3': 181}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:19:49,093] Trial 47 finished with value: 0.8500315576286693 and parameters: {'learning_rate': 0.0020165858943795275, 'batcher': False, 'n_layers': 5, 'n_dim_0': 416, 'n_dim_1': 107, 'n_dim_2': 199, 'n_dim_3': 174}. Best is trial 12 with value: 0.9316672875257372.\n",
      "[I 2023-12-08 22:21:17,553] Trial 48 finished with value: 0.9496958645762505 and parameters: {'learning_rate': 0.007698706581726475, 'batcher': False, 'n_layers': 4, 'n_dim_0': 344, 'n_dim_1': 50, 'n_dim_2': 294}. Best is trial 48 with value: 0.9496958645762505.\n",
      "[I 2023-12-08 22:22:45,587] Trial 49 finished with value: 0.8668898136524434 and parameters: {'learning_rate': 0.00312245126763305, 'batcher': False, 'n_layers': 4, 'n_dim_0': 381, 'n_dim_1': 40, 'n_dim_2': 309}. Best is trial 48 with value: 0.9496958645762505.\n"
     ]
    }
   ],
   "source": [
    "study_tree = optuna.create_study(study_name=\"Best tree label config\", directions=['maximize'])\n",
    "study_tree.optimize(objective, n_trials=n_trials, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T22:22:45.609952Z",
     "end_time": "2023-12-08T22:22:45.625364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.007698706581726475, 'batcher': False, 'n_layers': 4, 'n_dim_0': 344, 'n_dim_1': 50, 'n_dim_2': 294}\n"
     ]
    }
   ],
   "source": [
    "best_par_tree = study_tree.best_params\n",
    "print(best_par_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T22:22:45.622190Z",
     "end_time": "2023-12-08T22:22:45.646806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'TPESampler'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_tree.sampler.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T22:22:45.638361Z",
     "end_time": "2023-12-08T22:22:45.674285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    number       value             datetime_start          datetime_complete  \\\n0        0    0.866006 2023-12-08 21:20:08.788536 2023-12-08 21:21:18.473993   \n1        1    0.797204 2023-12-08 21:21:18.474991 2023-12-08 21:22:29.376470   \n2        2    0.813047 2023-12-08 21:22:29.377470 2023-12-08 21:23:45.153962   \n3        3    0.887530 2023-12-08 21:23:45.154963 2023-12-08 21:25:01.064178   \n4        4    0.765364 2023-12-08 21:25:01.065169 2023-12-08 21:26:13.469076   \n5        5 -749.315467 2023-12-08 21:26:13.471076 2023-12-08 21:27:22.298319   \n6        6 -245.449412 2023-12-08 21:27:22.299318 2023-12-08 21:28:35.022163   \n7        7 -643.681754 2023-12-08 21:28:35.024373 2023-12-08 21:29:48.098389   \n8        8 -740.846402 2023-12-08 21:29:48.099388 2023-12-08 21:31:05.536873   \n9        9    0.749897 2023-12-08 21:31:05.537872 2023-12-08 21:32:21.574676   \n10      10  -13.468859 2023-12-08 21:32:21.575638 2023-12-08 21:33:29.972476   \n11      11    0.837639 2023-12-08 21:33:29.974478 2023-12-08 21:34:43.950472   \n12      12    0.931667 2023-12-08 21:34:43.951475 2023-12-08 21:35:55.972492   \n13      13    0.851266 2023-12-08 21:35:55.973489 2023-12-08 21:37:12.130260   \n14      14    0.854453 2023-12-08 21:37:12.132261 2023-12-08 21:38:25.001637   \n15      15    0.821525 2023-12-08 21:38:25.002635 2023-12-08 21:39:33.694643   \n16      16   -0.029530 2023-12-08 21:39:33.696640 2023-12-08 21:40:46.712801   \n17      17    0.846315 2023-12-08 21:40:46.713801 2023-12-08 21:42:04.906719   \n18      18    0.798681 2023-12-08 21:42:04.907719 2023-12-08 21:43:25.486499   \n19      19    0.844226 2023-12-08 21:43:25.487519 2023-12-08 21:44:40.632539   \n20      20    0.824785 2023-12-08 21:44:40.634536 2023-12-08 21:45:53.613116   \n21      21    0.910767 2023-12-08 21:45:53.614115 2023-12-08 21:47:08.808191   \n22      22    0.869827 2023-12-08 21:47:08.809190 2023-12-08 21:48:26.825224   \n23      23    0.887536 2023-12-08 21:48:26.828226 2023-12-08 21:49:43.662796   \n24      24    0.801351 2023-12-08 21:49:43.664794 2023-12-08 21:50:55.312989   \n25      25    0.864123 2023-12-08 21:50:55.313989 2023-12-08 21:52:06.601549   \n26      26    0.921453 2023-12-08 21:52:06.603549 2023-12-08 21:53:21.133330   \n27      27    0.818736 2023-12-08 21:53:21.135330 2023-12-08 21:54:38.920671   \n28      28    0.805996 2023-12-08 21:54:38.922671 2023-12-08 21:55:53.215052   \n29      29    0.727278 2023-12-08 21:55:53.217051 2023-12-08 21:57:02.905770   \n30      30    0.877831 2023-12-08 21:57:02.907769 2023-12-08 21:58:09.876385   \n31      31    0.867115 2023-12-08 21:58:09.877386 2023-12-08 21:59:21.568161   \n32      32    0.873196 2023-12-08 21:59:21.569159 2023-12-08 22:00:38.177025   \n33      33    0.855848 2023-12-08 22:00:38.179024 2023-12-08 22:01:56.233926   \n34      34   -0.402203 2023-12-08 22:01:56.234927 2023-12-08 22:03:07.607395   \n35      35    0.863671 2023-12-08 22:03:07.608395 2023-12-08 22:04:20.411379   \n36      36    0.866602 2023-12-08 22:04:20.413379 2023-12-08 22:05:38.502934   \n37      37    0.810529 2023-12-08 22:05:38.503934 2023-12-08 22:07:00.850976   \n38      38    0.839970 2023-12-08 22:07:00.853364 2023-12-08 22:08:15.189347   \n39      39    0.714966 2023-12-08 22:08:15.190347 2023-12-08 22:09:27.022193   \n40      40    0.794804 2023-12-08 22:09:27.023191 2023-12-08 22:10:44.378363   \n41      41    0.825988 2023-12-08 22:10:44.379363 2023-12-08 22:12:04.288934   \n42      42    0.903723 2023-12-08 22:12:04.290935 2023-12-08 22:13:15.338374   \n43      43    0.891442 2023-12-08 22:13:15.339372 2023-12-08 22:14:31.522418   \n44      44    0.892876 2023-12-08 22:14:31.524415 2023-12-08 22:15:45.127188   \n45      45   -0.002770 2023-12-08 22:15:45.128180 2023-12-08 22:17:02.027211   \n46      46    0.812987 2023-12-08 22:17:02.029121 2023-12-08 22:18:21.891415   \n47      47    0.850032 2023-12-08 22:18:21.893414 2023-12-08 22:19:49.092493   \n48      48    0.949696 2023-12-08 22:19:49.094493 2023-12-08 22:21:17.553884   \n49      49    0.866890 2023-12-08 22:21:17.555883 2023-12-08 22:22:45.587497   \n\n                 duration  params_batcher  params_learning_rate  \\\n0  0 days 00:01:09.685457            True              0.008647   \n1  0 days 00:01:10.901479           False              0.000698   \n2  0 days 00:01:15.776492           False              0.004319   \n3  0 days 00:01:15.909215           False              0.006581   \n4  0 days 00:01:12.403907           False              0.000179   \n5  0 days 00:01:08.827243            True              0.000043   \n6  0 days 00:01:12.722845           False              0.000023   \n7  0 days 00:01:13.074016            True              0.000072   \n8  0 days 00:01:17.437485            True              0.000011   \n9  0 days 00:01:16.036804           False              0.000200   \n10 0 days 00:01:08.396838           False              0.001378   \n11 0 days 00:01:13.975994            True              0.009356   \n12 0 days 00:01:12.021017            True              0.009598   \n13 0 days 00:01:16.156771            True              0.002550   \n14 0 days 00:01:12.869376            True              0.002825   \n15 0 days 00:01:08.692008           False              0.000892   \n16 0 days 00:01:13.016161           False              0.009499   \n17 0 days 00:01:18.192918            True              0.004230   \n18 0 days 00:01:20.578780            True              0.000454   \n19 0 days 00:01:15.145020           False              0.001915   \n20 0 days 00:01:12.978580           False              0.004303   \n21 0 days 00:01:15.194076            True              0.008959   \n22 0 days 00:01:18.016034            True              0.006455   \n23 0 days 00:01:16.834570            True              0.004485   \n24 0 days 00:01:11.648195            True              0.001534   \n25 0 days 00:01:11.287560            True              0.003373   \n26 0 days 00:01:14.529781            True              0.005501   \n27 0 days 00:01:17.785341            True              0.009711   \n28 0 days 00:01:14.292381            True              0.002232   \n29 0 days 00:01:09.688719            True              0.006367   \n30 0 days 00:01:06.968616            True              0.006129   \n31 0 days 00:01:11.690775            True              0.004193   \n32 0 days 00:01:16.607866            True              0.005882   \n33 0 days 00:01:18.054902            True              0.002780   \n34 0 days 00:01:11.372468            True              0.009355   \n35 0 days 00:01:12.802984            True              0.004312   \n36 0 days 00:01:18.089555            True              0.006279   \n37 0 days 00:01:22.347042            True              0.001127   \n38 0 days 00:01:14.335983            True              0.001824   \n39 0 days 00:01:11.831846            True              0.003210   \n40 0 days 00:01:17.355172            True              0.000735   \n41 0 days 00:01:19.909571           False              0.006702   \n42 0 days 00:01:11.047439           False              0.005310   \n43 0 days 00:01:16.183046           False              0.004431   \n44 0 days 00:01:13.602773           False              0.007328   \n45 0 days 00:01:16.899031           False              0.009969   \n46 0 days 00:01:19.862294           False              0.007524   \n47 0 days 00:01:27.199079           False              0.002017   \n48 0 days 00:01:28.459391           False              0.007699   \n49 0 days 00:01:28.031614           False              0.003122   \n\n    params_n_dim_0  params_n_dim_1  params_n_dim_2  params_n_dim_3  \\\n0            181.0           132.0           125.0             NaN   \n1            190.0             NaN             NaN             NaN   \n2            403.0           324.0           495.0             NaN   \n3            274.0            11.0           427.0             NaN   \n4            441.0             NaN             NaN             NaN   \n5              NaN             NaN             NaN             NaN   \n6            219.0           420.0           163.0           378.0   \n7              NaN             NaN             NaN             NaN   \n8            494.0            42.0             NaN             NaN   \n9            257.0           185.0           423.0           352.0   \n10            15.0             1.0           331.0             NaN   \n11           111.0           145.0            33.0             NaN   \n12           330.0           134.0             NaN             NaN   \n13           334.0            85.0             NaN             NaN   \n14           308.0             NaN             NaN             NaN   \n15           361.0           255.0             NaN             NaN   \n16           298.0           227.0           314.0             5.0   \n17           118.0            85.0           510.0             NaN   \n18           377.0            10.0             NaN             NaN   \n19           271.0             NaN             NaN             NaN   \n20           457.0           319.0           365.0             NaN   \n21           175.0           142.0             NaN             NaN   \n22           154.0           122.0             NaN             NaN   \n23           216.0           193.0             NaN             NaN   \n24            76.0             NaN             NaN             NaN   \n25           224.0           192.0             NaN             NaN   \n26           228.0           299.0             NaN             NaN   \n27           150.0           309.0             NaN             NaN   \n28            66.0             NaN             NaN             NaN   \n29           325.0           437.0             NaN             NaN   \n30           238.0             NaN             NaN             NaN   \n31           202.0           214.0             NaN             NaN   \n32           159.0           279.0             NaN             NaN   \n33           187.0           362.0             NaN             NaN   \n34           252.0           512.0           211.0             NaN   \n35           184.0             NaN             NaN             NaN   \n36           280.0           153.0             8.0             NaN   \n37           354.0            87.0             NaN             NaN   \n38           399.0             NaN             NaN             NaN   \n39             NaN             NaN             NaN             NaN   \n40           118.0           174.0             NaN             NaN   \n41           213.0           233.0           397.0             NaN   \n42           278.0            50.0           270.0             NaN   \n43           294.0           117.0           258.0             NaN   \n44           302.0           116.0           263.0            90.0   \n45           326.0            67.0           257.0            81.0   \n46           256.0            47.0           106.0           181.0   \n47           416.0           107.0           199.0           174.0   \n48           344.0            50.0           294.0             NaN   \n49           381.0            40.0           309.0             NaN   \n\n    params_n_layers     state  \n0                 4  COMPLETE  \n1                 2  COMPLETE  \n2                 4  COMPLETE  \n3                 4  COMPLETE  \n4                 2  COMPLETE  \n5                 1  COMPLETE  \n6                 5  COMPLETE  \n7                 1  COMPLETE  \n8                 3  COMPLETE  \n9                 5  COMPLETE  \n10                4  COMPLETE  \n11                4  COMPLETE  \n12                3  COMPLETE  \n13                3  COMPLETE  \n14                2  COMPLETE  \n15                3  COMPLETE  \n16                5  COMPLETE  \n17                4  COMPLETE  \n18                3  COMPLETE  \n19                2  COMPLETE  \n20                4  COMPLETE  \n21                3  COMPLETE  \n22                3  COMPLETE  \n23                3  COMPLETE  \n24                2  COMPLETE  \n25                3  COMPLETE  \n26                3  COMPLETE  \n27                3  COMPLETE  \n28                2  COMPLETE  \n29                3  COMPLETE  \n30                2  COMPLETE  \n31                3  COMPLETE  \n32                3  COMPLETE  \n33                3  COMPLETE  \n34                4  COMPLETE  \n35                2  COMPLETE  \n36                4  COMPLETE  \n37                3  COMPLETE  \n38                2  COMPLETE  \n39                1  COMPLETE  \n40                3  COMPLETE  \n41                4  COMPLETE  \n42                4  COMPLETE  \n43                4  COMPLETE  \n44                5  COMPLETE  \n45                5  COMPLETE  \n46                5  COMPLETE  \n47                5  COMPLETE  \n48                4  COMPLETE  \n49                4  COMPLETE  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number</th>\n      <th>value</th>\n      <th>datetime_start</th>\n      <th>datetime_complete</th>\n      <th>duration</th>\n      <th>params_batcher</th>\n      <th>params_learning_rate</th>\n      <th>params_n_dim_0</th>\n      <th>params_n_dim_1</th>\n      <th>params_n_dim_2</th>\n      <th>params_n_dim_3</th>\n      <th>params_n_layers</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.866006</td>\n      <td>2023-12-08 21:20:08.788536</td>\n      <td>2023-12-08 21:21:18.473993</td>\n      <td>0 days 00:01:09.685457</td>\n      <td>True</td>\n      <td>0.008647</td>\n      <td>181.0</td>\n      <td>132.0</td>\n      <td>125.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.797204</td>\n      <td>2023-12-08 21:21:18.474991</td>\n      <td>2023-12-08 21:22:29.376470</td>\n      <td>0 days 00:01:10.901479</td>\n      <td>False</td>\n      <td>0.000698</td>\n      <td>190.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.813047</td>\n      <td>2023-12-08 21:22:29.377470</td>\n      <td>2023-12-08 21:23:45.153962</td>\n      <td>0 days 00:01:15.776492</td>\n      <td>False</td>\n      <td>0.004319</td>\n      <td>403.0</td>\n      <td>324.0</td>\n      <td>495.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.887530</td>\n      <td>2023-12-08 21:23:45.154963</td>\n      <td>2023-12-08 21:25:01.064178</td>\n      <td>0 days 00:01:15.909215</td>\n      <td>False</td>\n      <td>0.006581</td>\n      <td>274.0</td>\n      <td>11.0</td>\n      <td>427.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.765364</td>\n      <td>2023-12-08 21:25:01.065169</td>\n      <td>2023-12-08 21:26:13.469076</td>\n      <td>0 days 00:01:12.403907</td>\n      <td>False</td>\n      <td>0.000179</td>\n      <td>441.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>-749.315467</td>\n      <td>2023-12-08 21:26:13.471076</td>\n      <td>2023-12-08 21:27:22.298319</td>\n      <td>0 days 00:01:08.827243</td>\n      <td>True</td>\n      <td>0.000043</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>-245.449412</td>\n      <td>2023-12-08 21:27:22.299318</td>\n      <td>2023-12-08 21:28:35.022163</td>\n      <td>0 days 00:01:12.722845</td>\n      <td>False</td>\n      <td>0.000023</td>\n      <td>219.0</td>\n      <td>420.0</td>\n      <td>163.0</td>\n      <td>378.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>-643.681754</td>\n      <td>2023-12-08 21:28:35.024373</td>\n      <td>2023-12-08 21:29:48.098389</td>\n      <td>0 days 00:01:13.074016</td>\n      <td>True</td>\n      <td>0.000072</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>-740.846402</td>\n      <td>2023-12-08 21:29:48.099388</td>\n      <td>2023-12-08 21:31:05.536873</td>\n      <td>0 days 00:01:17.437485</td>\n      <td>True</td>\n      <td>0.000011</td>\n      <td>494.0</td>\n      <td>42.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.749897</td>\n      <td>2023-12-08 21:31:05.537872</td>\n      <td>2023-12-08 21:32:21.574676</td>\n      <td>0 days 00:01:16.036804</td>\n      <td>False</td>\n      <td>0.000200</td>\n      <td>257.0</td>\n      <td>185.0</td>\n      <td>423.0</td>\n      <td>352.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>-13.468859</td>\n      <td>2023-12-08 21:32:21.575638</td>\n      <td>2023-12-08 21:33:29.972476</td>\n      <td>0 days 00:01:08.396838</td>\n      <td>False</td>\n      <td>0.001378</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>331.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>0.837639</td>\n      <td>2023-12-08 21:33:29.974478</td>\n      <td>2023-12-08 21:34:43.950472</td>\n      <td>0 days 00:01:13.975994</td>\n      <td>True</td>\n      <td>0.009356</td>\n      <td>111.0</td>\n      <td>145.0</td>\n      <td>33.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>0.931667</td>\n      <td>2023-12-08 21:34:43.951475</td>\n      <td>2023-12-08 21:35:55.972492</td>\n      <td>0 days 00:01:12.021017</td>\n      <td>True</td>\n      <td>0.009598</td>\n      <td>330.0</td>\n      <td>134.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>0.851266</td>\n      <td>2023-12-08 21:35:55.973489</td>\n      <td>2023-12-08 21:37:12.130260</td>\n      <td>0 days 00:01:16.156771</td>\n      <td>True</td>\n      <td>0.002550</td>\n      <td>334.0</td>\n      <td>85.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>0.854453</td>\n      <td>2023-12-08 21:37:12.132261</td>\n      <td>2023-12-08 21:38:25.001637</td>\n      <td>0 days 00:01:12.869376</td>\n      <td>True</td>\n      <td>0.002825</td>\n      <td>308.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>0.821525</td>\n      <td>2023-12-08 21:38:25.002635</td>\n      <td>2023-12-08 21:39:33.694643</td>\n      <td>0 days 00:01:08.692008</td>\n      <td>False</td>\n      <td>0.000892</td>\n      <td>361.0</td>\n      <td>255.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>-0.029530</td>\n      <td>2023-12-08 21:39:33.696640</td>\n      <td>2023-12-08 21:40:46.712801</td>\n      <td>0 days 00:01:13.016161</td>\n      <td>False</td>\n      <td>0.009499</td>\n      <td>298.0</td>\n      <td>227.0</td>\n      <td>314.0</td>\n      <td>5.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>0.846315</td>\n      <td>2023-12-08 21:40:46.713801</td>\n      <td>2023-12-08 21:42:04.906719</td>\n      <td>0 days 00:01:18.192918</td>\n      <td>True</td>\n      <td>0.004230</td>\n      <td>118.0</td>\n      <td>85.0</td>\n      <td>510.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>0.798681</td>\n      <td>2023-12-08 21:42:04.907719</td>\n      <td>2023-12-08 21:43:25.486499</td>\n      <td>0 days 00:01:20.578780</td>\n      <td>True</td>\n      <td>0.000454</td>\n      <td>377.0</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>0.844226</td>\n      <td>2023-12-08 21:43:25.487519</td>\n      <td>2023-12-08 21:44:40.632539</td>\n      <td>0 days 00:01:15.145020</td>\n      <td>False</td>\n      <td>0.001915</td>\n      <td>271.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>0.824785</td>\n      <td>2023-12-08 21:44:40.634536</td>\n      <td>2023-12-08 21:45:53.613116</td>\n      <td>0 days 00:01:12.978580</td>\n      <td>False</td>\n      <td>0.004303</td>\n      <td>457.0</td>\n      <td>319.0</td>\n      <td>365.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>0.910767</td>\n      <td>2023-12-08 21:45:53.614115</td>\n      <td>2023-12-08 21:47:08.808191</td>\n      <td>0 days 00:01:15.194076</td>\n      <td>True</td>\n      <td>0.008959</td>\n      <td>175.0</td>\n      <td>142.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>0.869827</td>\n      <td>2023-12-08 21:47:08.809190</td>\n      <td>2023-12-08 21:48:26.825224</td>\n      <td>0 days 00:01:18.016034</td>\n      <td>True</td>\n      <td>0.006455</td>\n      <td>154.0</td>\n      <td>122.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>0.887536</td>\n      <td>2023-12-08 21:48:26.828226</td>\n      <td>2023-12-08 21:49:43.662796</td>\n      <td>0 days 00:01:16.834570</td>\n      <td>True</td>\n      <td>0.004485</td>\n      <td>216.0</td>\n      <td>193.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>0.801351</td>\n      <td>2023-12-08 21:49:43.664794</td>\n      <td>2023-12-08 21:50:55.312989</td>\n      <td>0 days 00:01:11.648195</td>\n      <td>True</td>\n      <td>0.001534</td>\n      <td>76.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>0.864123</td>\n      <td>2023-12-08 21:50:55.313989</td>\n      <td>2023-12-08 21:52:06.601549</td>\n      <td>0 days 00:01:11.287560</td>\n      <td>True</td>\n      <td>0.003373</td>\n      <td>224.0</td>\n      <td>192.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>0.921453</td>\n      <td>2023-12-08 21:52:06.603549</td>\n      <td>2023-12-08 21:53:21.133330</td>\n      <td>0 days 00:01:14.529781</td>\n      <td>True</td>\n      <td>0.005501</td>\n      <td>228.0</td>\n      <td>299.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>0.818736</td>\n      <td>2023-12-08 21:53:21.135330</td>\n      <td>2023-12-08 21:54:38.920671</td>\n      <td>0 days 00:01:17.785341</td>\n      <td>True</td>\n      <td>0.009711</td>\n      <td>150.0</td>\n      <td>309.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>0.805996</td>\n      <td>2023-12-08 21:54:38.922671</td>\n      <td>2023-12-08 21:55:53.215052</td>\n      <td>0 days 00:01:14.292381</td>\n      <td>True</td>\n      <td>0.002232</td>\n      <td>66.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>0.727278</td>\n      <td>2023-12-08 21:55:53.217051</td>\n      <td>2023-12-08 21:57:02.905770</td>\n      <td>0 days 00:01:09.688719</td>\n      <td>True</td>\n      <td>0.006367</td>\n      <td>325.0</td>\n      <td>437.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>30</td>\n      <td>0.877831</td>\n      <td>2023-12-08 21:57:02.907769</td>\n      <td>2023-12-08 21:58:09.876385</td>\n      <td>0 days 00:01:06.968616</td>\n      <td>True</td>\n      <td>0.006129</td>\n      <td>238.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>31</td>\n      <td>0.867115</td>\n      <td>2023-12-08 21:58:09.877386</td>\n      <td>2023-12-08 21:59:21.568161</td>\n      <td>0 days 00:01:11.690775</td>\n      <td>True</td>\n      <td>0.004193</td>\n      <td>202.0</td>\n      <td>214.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>0.873196</td>\n      <td>2023-12-08 21:59:21.569159</td>\n      <td>2023-12-08 22:00:38.177025</td>\n      <td>0 days 00:01:16.607866</td>\n      <td>True</td>\n      <td>0.005882</td>\n      <td>159.0</td>\n      <td>279.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>33</td>\n      <td>0.855848</td>\n      <td>2023-12-08 22:00:38.179024</td>\n      <td>2023-12-08 22:01:56.233926</td>\n      <td>0 days 00:01:18.054902</td>\n      <td>True</td>\n      <td>0.002780</td>\n      <td>187.0</td>\n      <td>362.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>34</td>\n      <td>-0.402203</td>\n      <td>2023-12-08 22:01:56.234927</td>\n      <td>2023-12-08 22:03:07.607395</td>\n      <td>0 days 00:01:11.372468</td>\n      <td>True</td>\n      <td>0.009355</td>\n      <td>252.0</td>\n      <td>512.0</td>\n      <td>211.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>35</td>\n      <td>0.863671</td>\n      <td>2023-12-08 22:03:07.608395</td>\n      <td>2023-12-08 22:04:20.411379</td>\n      <td>0 days 00:01:12.802984</td>\n      <td>True</td>\n      <td>0.004312</td>\n      <td>184.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>36</td>\n      <td>0.866602</td>\n      <td>2023-12-08 22:04:20.413379</td>\n      <td>2023-12-08 22:05:38.502934</td>\n      <td>0 days 00:01:18.089555</td>\n      <td>True</td>\n      <td>0.006279</td>\n      <td>280.0</td>\n      <td>153.0</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>37</td>\n      <td>0.810529</td>\n      <td>2023-12-08 22:05:38.503934</td>\n      <td>2023-12-08 22:07:00.850976</td>\n      <td>0 days 00:01:22.347042</td>\n      <td>True</td>\n      <td>0.001127</td>\n      <td>354.0</td>\n      <td>87.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>38</td>\n      <td>0.839970</td>\n      <td>2023-12-08 22:07:00.853364</td>\n      <td>2023-12-08 22:08:15.189347</td>\n      <td>0 days 00:01:14.335983</td>\n      <td>True</td>\n      <td>0.001824</td>\n      <td>399.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>39</td>\n      <td>0.714966</td>\n      <td>2023-12-08 22:08:15.190347</td>\n      <td>2023-12-08 22:09:27.022193</td>\n      <td>0 days 00:01:11.831846</td>\n      <td>True</td>\n      <td>0.003210</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>40</td>\n      <td>0.794804</td>\n      <td>2023-12-08 22:09:27.023191</td>\n      <td>2023-12-08 22:10:44.378363</td>\n      <td>0 days 00:01:17.355172</td>\n      <td>True</td>\n      <td>0.000735</td>\n      <td>118.0</td>\n      <td>174.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>0.825988</td>\n      <td>2023-12-08 22:10:44.379363</td>\n      <td>2023-12-08 22:12:04.288934</td>\n      <td>0 days 00:01:19.909571</td>\n      <td>False</td>\n      <td>0.006702</td>\n      <td>213.0</td>\n      <td>233.0</td>\n      <td>397.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>42</td>\n      <td>0.903723</td>\n      <td>2023-12-08 22:12:04.290935</td>\n      <td>2023-12-08 22:13:15.338374</td>\n      <td>0 days 00:01:11.047439</td>\n      <td>False</td>\n      <td>0.005310</td>\n      <td>278.0</td>\n      <td>50.0</td>\n      <td>270.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>43</td>\n      <td>0.891442</td>\n      <td>2023-12-08 22:13:15.339372</td>\n      <td>2023-12-08 22:14:31.522418</td>\n      <td>0 days 00:01:16.183046</td>\n      <td>False</td>\n      <td>0.004431</td>\n      <td>294.0</td>\n      <td>117.0</td>\n      <td>258.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>44</td>\n      <td>0.892876</td>\n      <td>2023-12-08 22:14:31.524415</td>\n      <td>2023-12-08 22:15:45.127188</td>\n      <td>0 days 00:01:13.602773</td>\n      <td>False</td>\n      <td>0.007328</td>\n      <td>302.0</td>\n      <td>116.0</td>\n      <td>263.0</td>\n      <td>90.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>-0.002770</td>\n      <td>2023-12-08 22:15:45.128180</td>\n      <td>2023-12-08 22:17:02.027211</td>\n      <td>0 days 00:01:16.899031</td>\n      <td>False</td>\n      <td>0.009969</td>\n      <td>326.0</td>\n      <td>67.0</td>\n      <td>257.0</td>\n      <td>81.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>46</td>\n      <td>0.812987</td>\n      <td>2023-12-08 22:17:02.029121</td>\n      <td>2023-12-08 22:18:21.891415</td>\n      <td>0 days 00:01:19.862294</td>\n      <td>False</td>\n      <td>0.007524</td>\n      <td>256.0</td>\n      <td>47.0</td>\n      <td>106.0</td>\n      <td>181.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>47</td>\n      <td>0.850032</td>\n      <td>2023-12-08 22:18:21.893414</td>\n      <td>2023-12-08 22:19:49.092493</td>\n      <td>0 days 00:01:27.199079</td>\n      <td>False</td>\n      <td>0.002017</td>\n      <td>416.0</td>\n      <td>107.0</td>\n      <td>199.0</td>\n      <td>174.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>48</td>\n      <td>0.949696</td>\n      <td>2023-12-08 22:19:49.094493</td>\n      <td>2023-12-08 22:21:17.553884</td>\n      <td>0 days 00:01:28.459391</td>\n      <td>False</td>\n      <td>0.007699</td>\n      <td>344.0</td>\n      <td>50.0</td>\n      <td>294.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>49</td>\n      <td>0.866890</td>\n      <td>2023-12-08 22:21:17.555883</td>\n      <td>2023-12-08 22:22:45.587497</td>\n      <td>0 days 00:01:28.031614</td>\n      <td>False</td>\n      <td>0.003122</td>\n      <td>381.0</td>\n      <td>40.0</td>\n      <td>309.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tree = study_tree.trials_dataframe()\n",
    "df_tree.to_csv(\"study_label-tree_baseline_normalized.csv\")\n",
    "display(df_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-09T08:10:21.515227Z",
     "end_time": "2023-12-09T08:10:21.532291Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting global parameters\n",
    "dataset_path = \"../../datasets/\"\n",
    "label = \"default\" # alt: tree or default\n",
    "test_perc = 0.3\n",
    "val_perc = 0.21\n",
    "stability_count = 5\n",
    "n_trials = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-09T08:10:22.872619Z",
     "end_time": "2023-12-09T09:24:09.329350Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 08:10:22,872] A new study created in memory with name: Best default label config\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5b6694549d045ba93612d892a1f9614"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 08:11:32,361] Trial 0 finished with value: -3.898854506925217 and parameters: {'learning_rate': 0.006544251119539077, 'batcher': True, 'n_layers': 3, 'n_dim_0': 421, 'n_dim_1': 91}. Best is trial 0 with value: -3.898854506925217.\n",
      "[I 2023-12-09 08:12:36,517] Trial 1 finished with value: -994.8309539800778 and parameters: {'learning_rate': 5.4461236965534135e-05, 'batcher': False, 'n_layers': 3, 'n_dim_0': 342, 'n_dim_1': 253}. Best is trial 0 with value: -3.898854506925217.\n",
      "[I 2023-12-09 08:13:57,499] Trial 2 finished with value: -5.316374556922454 and parameters: {'learning_rate': 0.00017300771364700649, 'batcher': True, 'n_layers': 4, 'n_dim_0': 378, 'n_dim_1': 388, 'n_dim_2': 368}. Best is trial 0 with value: -3.898854506925217.\n",
      "[I 2023-12-09 08:15:25,417] Trial 3 finished with value: -4.948884924978612 and parameters: {'learning_rate': 0.00022589940143170241, 'batcher': False, 'n_layers': 3, 'n_dim_0': 505, 'n_dim_1': 177}. Best is trial 0 with value: -3.898854506925217.\n",
      "[I 2023-12-09 08:16:59,327] Trial 4 finished with value: -3.602496035532466 and parameters: {'learning_rate': 0.003004424703176275, 'batcher': False, 'n_layers': 4, 'n_dim_0': 295, 'n_dim_1': 296, 'n_dim_2': 157}. Best is trial 4 with value: -3.602496035532466.\n",
      "[I 2023-12-09 08:18:28,096] Trial 5 finished with value: -224.82862905743536 and parameters: {'learning_rate': 0.002097639389493674, 'batcher': True, 'n_layers': 1}. Best is trial 4 with value: -3.602496035532466.\n",
      "[I 2023-12-09 08:19:55,976] Trial 6 finished with value: -4685.181908912513 and parameters: {'learning_rate': 1.6080708738378273e-05, 'batcher': False, 'n_layers': 4, 'n_dim_0': 79, 'n_dim_1': 309, 'n_dim_2': 186}. Best is trial 4 with value: -3.602496035532466.\n",
      "[I 2023-12-09 08:21:25,292] Trial 7 finished with value: -4.801416098630096 and parameters: {'learning_rate': 0.0016570958685605036, 'batcher': True, 'n_layers': 4, 'n_dim_0': 19, 'n_dim_1': 226, 'n_dim_2': 496}. Best is trial 4 with value: -3.602496035532466.\n",
      "[I 2023-12-09 08:22:57,063] Trial 8 finished with value: -19.34066581952083 and parameters: {'learning_rate': 0.0001863125320937225, 'batcher': False, 'n_layers': 3, 'n_dim_0': 135, 'n_dim_1': 48}. Best is trial 4 with value: -3.602496035532466.\n",
      "[I 2023-12-09 08:24:29,571] Trial 9 finished with value: -4561.963113232198 and parameters: {'learning_rate': 1.1426454790788674e-05, 'batcher': False, 'n_layers': 4, 'n_dim_0': 296, 'n_dim_1': 502, 'n_dim_2': 253}. Best is trial 4 with value: -3.602496035532466.\n",
      "[I 2023-12-09 08:25:59,999] Trial 10 finished with value: -0.021934404549128005 and parameters: {'learning_rate': 0.009049448041587074, 'batcher': False, 'n_layers': 5, 'n_dim_0': 207, 'n_dim_1': 411, 'n_dim_2': 12, 'n_dim_3': 505}. Best is trial 10 with value: -0.021934404549128005.\n",
      "[I 2023-12-09 08:27:27,700] Trial 11 finished with value: -0.052663660770160804 and parameters: {'learning_rate': 0.009828811749347466, 'batcher': False, 'n_layers': 5, 'n_dim_0': 203, 'n_dim_1': 402, 'n_dim_2': 11, 'n_dim_3': 496}. Best is trial 10 with value: -0.021934404549128005.\n",
      "[I 2023-12-09 08:28:48,599] Trial 12 finished with value: -0.019113087890416613 and parameters: {'learning_rate': 0.008066653181324126, 'batcher': False, 'n_layers': 5, 'n_dim_0': 187, 'n_dim_1': 434, 'n_dim_2': 1, 'n_dim_3': 511}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:30:10,127] Trial 13 finished with value: -4.502964002532938 and parameters: {'learning_rate': 0.0007734033547313883, 'batcher': False, 'n_layers': 5, 'n_dim_0': 195, 'n_dim_1': 489, 'n_dim_2': 27, 'n_dim_3': 509}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:31:32,641] Trial 14 finished with value: -2.381544927708904 and parameters: {'learning_rate': 0.009688743893786081, 'batcher': False, 'n_layers': 5, 'n_dim_0': 203, 'n_dim_1': 407, 'n_dim_2': 85, 'n_dim_3': 18}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:32:57,617] Trial 15 finished with value: -4.986612083126256 and parameters: {'learning_rate': 0.0048337940656720466, 'batcher': False, 'n_layers': 1}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:34:16,971] Trial 16 finished with value: -4.8550342365628065 and parameters: {'learning_rate': 0.0010290838091318996, 'batcher': False, 'n_layers': 2, 'n_dim_0': 131}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:35:43,552] Trial 17 finished with value: -3.6294551344956867 and parameters: {'learning_rate': 0.0035387022696056277, 'batcher': False, 'n_layers': 5, 'n_dim_0': 239, 'n_dim_1': 348, 'n_dim_2': 94, 'n_dim_3': 346}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:37:13,068] Trial 18 finished with value: -4.689031762579506 and parameters: {'learning_rate': 0.004747166040854861, 'batcher': True, 'n_layers': 2, 'n_dim_0': 134}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:38:40,343] Trial 19 finished with value: -4.77498791376089 and parameters: {'learning_rate': 0.0008995526242824257, 'batcher': False, 'n_layers': 5, 'n_dim_0': 24, 'n_dim_1': 452, 'n_dim_2': 9, 'n_dim_3': 354}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:40:07,487] Trial 20 finished with value: -4.599144771324489 and parameters: {'learning_rate': 0.0004844143758748274, 'batcher': False, 'n_layers': 5, 'n_dim_0': 271, 'n_dim_1': 440, 'n_dim_2': 335, 'n_dim_3': 432}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:41:36,659] Trial 21 finished with value: -3.4017422331764267 and parameters: {'learning_rate': 0.009529903138599194, 'batcher': False, 'n_layers': 5, 'n_dim_0': 182, 'n_dim_1': 361, 'n_dim_2': 14, 'n_dim_3': 511}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:43:09,229] Trial 22 finished with value: -4.280439351255785 and parameters: {'learning_rate': 0.008455913789555401, 'batcher': False, 'n_layers': 5, 'n_dim_0': 239, 'n_dim_1': 444, 'n_dim_2': 99, 'n_dim_3': 507}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:44:38,895] Trial 23 finished with value: -4.379695641492566 and parameters: {'learning_rate': 0.0030896222879245993, 'batcher': False, 'n_layers': 4, 'n_dim_0': 81, 'n_dim_1': 327, 'n_dim_2': 4}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:46:10,565] Trial 24 finished with value: -3.678908917684823 and parameters: {'learning_rate': 0.006140084215649302, 'batcher': False, 'n_layers': 5, 'n_dim_0': 166, 'n_dim_1': 412, 'n_dim_2': 83, 'n_dim_3': 124}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:47:46,175] Trial 25 finished with value: -3.814435015627459 and parameters: {'learning_rate': 0.0020631146417513053, 'batcher': False, 'n_layers': 4, 'n_dim_0': 238, 'n_dim_1': 508, 'n_dim_2': 154}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:49:15,640] Trial 26 finished with value: -3.3497261174819757 and parameters: {'learning_rate': 0.005981483543336197, 'batcher': True, 'n_layers': 5, 'n_dim_0': 340, 'n_dim_1': 375, 'n_dim_2': 60, 'n_dim_3': 388}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:50:44,761] Trial 27 finished with value: -3.940457728607762 and parameters: {'learning_rate': 0.009655773597173348, 'batcher': False, 'n_layers': 2, 'n_dim_0': 93}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:52:17,164] Trial 28 finished with value: -3.7426817603701465 and parameters: {'learning_rate': 0.003981257849671706, 'batcher': False, 'n_layers': 5, 'n_dim_0': 212, 'n_dim_1': 453, 'n_dim_2': 138, 'n_dim_3': 253}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:53:45,203] Trial 29 finished with value: -4.024967273462434 and parameters: {'learning_rate': 0.005960867952644619, 'batcher': True, 'n_layers': 3, 'n_dim_0': 142, 'n_dim_1': 299}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:55:17,303] Trial 30 finished with value: -4.267322076734324 and parameters: {'learning_rate': 0.001533328729610005, 'batcher': False, 'n_layers': 4, 'n_dim_0': 283, 'n_dim_1': 178, 'n_dim_2': 215}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:56:47,867] Trial 31 finished with value: -2.357123711742294 and parameters: {'learning_rate': 0.00938786283683738, 'batcher': False, 'n_layers': 5, 'n_dim_0': 209, 'n_dim_1': 394, 'n_dim_2': 55, 'n_dim_3': 20}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:58:18,896] Trial 32 finished with value: -3.237243146554249 and parameters: {'learning_rate': 0.006047570707304849, 'batcher': False, 'n_layers': 5, 'n_dim_0': 168, 'n_dim_1': 401, 'n_dim_2': 55, 'n_dim_3': 223}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 08:59:52,856] Trial 33 finished with value: -3.461568410076269 and parameters: {'learning_rate': 0.007067228070295072, 'batcher': False, 'n_layers': 5, 'n_dim_0': 212, 'n_dim_1': 472, 'n_dim_2': 50, 'n_dim_3': 448}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:01:24,799] Trial 34 finished with value: -3.9484500738844224 and parameters: {'learning_rate': 0.00332231382248966, 'batcher': False, 'n_layers': 4, 'n_dim_0': 315, 'n_dim_1': 418, 'n_dim_2': 5}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:02:56,244] Trial 35 finished with value: -3.557335682066045 and parameters: {'learning_rate': 0.004933739244122689, 'batcher': False, 'n_layers': 5, 'n_dim_0': 410, 'n_dim_1': 359, 'n_dim_2': 130, 'n_dim_3': 17}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:04:25,468] Trial 36 finished with value: -4.323387025800567 and parameters: {'learning_rate': 0.002534710432964355, 'batcher': True, 'n_layers': 4, 'n_dim_0': 248, 'n_dim_1': 427, 'n_dim_2': 48}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:05:56,879] Trial 37 finished with value: -3.096506813438048 and parameters: {'learning_rate': 0.009703351820519525, 'batcher': False, 'n_layers': 5, 'n_dim_0': 111, 'n_dim_1': 340, 'n_dim_2': 114, 'n_dim_3': 433}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:07:28,320] Trial 38 finished with value: -3.612551001920056 and parameters: {'learning_rate': 0.0039151522824888395, 'batcher': False, 'n_layers': 4, 'n_dim_0': 496, 'n_dim_1': 261, 'n_dim_2': 47}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:08:55,552] Trial 39 finished with value: -3.864761849842118 and parameters: {'learning_rate': 0.0067157964658621965, 'batcher': True, 'n_layers': 5, 'n_dim_0': 157, 'n_dim_1': 388, 'n_dim_2': 2, 'n_dim_3': 167}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:10:25,636] Trial 40 finished with value: -4.330948113503577 and parameters: {'learning_rate': 0.0022742173055443557, 'batcher': False, 'n_layers': 3, 'n_dim_0': 60, 'n_dim_1': 473}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:11:56,082] Trial 41 finished with value: -1.576747889599035 and parameters: {'learning_rate': 0.009958431006081167, 'batcher': False, 'n_layers': 5, 'n_dim_0': 211, 'n_dim_1': 394, 'n_dim_2': 76, 'n_dim_3': 1}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:13:27,845] Trial 42 finished with value: -4.940520032871427 and parameters: {'learning_rate': 0.007636911930313789, 'batcher': False, 'n_layers': 5, 'n_dim_0': 226, 'n_dim_1': 376, 'n_dim_2': 327, 'n_dim_3': 69}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:14:59,575] Trial 43 finished with value: -3.6381971185784785 and parameters: {'learning_rate': 0.004351919688037807, 'batcher': False, 'n_layers': 4, 'n_dim_0': 263, 'n_dim_1': 126, 'n_dim_2': 67}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:16:29,778] Trial 44 finished with value: -3.714808848976557 and parameters: {'learning_rate': 0.007628310611115533, 'batcher': False, 'n_layers': 5, 'n_dim_0': 196, 'n_dim_1': 263, 'n_dim_2': 39, 'n_dim_3': 2}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:18:02,774] Trial 45 finished with value: -3.259948853199655 and parameters: {'learning_rate': 0.009918117336559428, 'batcher': False, 'n_layers': 5, 'n_dim_0': 182, 'n_dim_1': 324, 'n_dim_2': 454, 'n_dim_3': 66}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:19:35,916] Trial 46 finished with value: -3.314278274443182 and parameters: {'learning_rate': 0.002775621109817641, 'batcher': False, 'n_layers': 5, 'n_dim_0': 218, 'n_dim_1': 470, 'n_dim_2': 184, 'n_dim_3': 461}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:21:06,924] Trial 47 finished with value: -3.3764393516141533 and parameters: {'learning_rate': 0.0050595594814316575, 'batcher': False, 'n_layers': 4, 'n_dim_0': 304, 'n_dim_1': 384, 'n_dim_2': 32}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:22:38,828] Trial 48 finished with value: -3.848851663178827 and parameters: {'learning_rate': 0.0038771420365805048, 'batcher': False, 'n_layers': 5, 'n_dim_0': 327, 'n_dim_1': 10, 'n_dim_2': 72, 'n_dim_3': 469}. Best is trial 12 with value: -0.019113087890416613.\n",
      "[I 2023-12-09 09:24:09,322] Trial 49 finished with value: -5.461334668329734 and parameters: {'learning_rate': 0.007125637286174282, 'batcher': True, 'n_layers': 1}. Best is trial 12 with value: -0.019113087890416613.\n"
     ]
    }
   ],
   "source": [
    "study_default = optuna.create_study(study_name=\"Best default label config\", directions=['maximize'])\n",
    "study_default.optimize(objective, n_trials=n_trials, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-09T09:24:09.330350Z",
     "end_time": "2023-12-09T09:24:09.345224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.008066653181324126, 'batcher': False, 'n_layers': 5, 'n_dim_0': 187, 'n_dim_1': 434, 'n_dim_2': 1, 'n_dim_3': 511}\n"
     ]
    }
   ],
   "source": [
    "best_par_default = study_default.best_params\n",
    "print(best_par_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-09T09:24:09.347188Z",
     "end_time": "2023-12-09T09:24:09.404501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'TPESampler'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_default.sampler.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-09T09:24:09.361278Z",
     "end_time": "2023-12-09T09:24:09.425521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    number        value             datetime_start          datetime_complete  \\\n0        0    -3.898855 2023-12-09 08:10:22.880619 2023-12-09 08:11:32.361798   \n1        1  -994.830954 2023-12-09 08:11:32.363805 2023-12-09 08:12:36.517840   \n2        2    -5.316375 2023-12-09 08:12:36.519843 2023-12-09 08:13:57.498733   \n3        3    -4.948885 2023-12-09 08:13:57.501733 2023-12-09 08:15:25.417381   \n4        4    -3.602496 2023-12-09 08:15:25.419382 2023-12-09 08:16:59.327848   \n5        5  -224.828629 2023-12-09 08:16:59.328859 2023-12-09 08:18:28.096367   \n6        6 -4685.181909 2023-12-09 08:18:28.097373 2023-12-09 08:19:55.975284   \n7        7    -4.801416 2023-12-09 08:19:55.977283 2023-12-09 08:21:25.292594   \n8        8   -19.340666 2023-12-09 08:21:25.294594 2023-12-09 08:22:57.062641   \n9        9 -4561.963113 2023-12-09 08:22:57.064599 2023-12-09 08:24:29.571116   \n10      10    -0.021934 2023-12-09 08:24:29.572116 2023-12-09 08:25:59.999904   \n11      11    -0.052664 2023-12-09 08:26:00.001160 2023-12-09 08:27:27.699607   \n12      12    -0.019113 2023-12-09 08:27:27.701608 2023-12-09 08:28:48.599848   \n13      13    -4.502964 2023-12-09 08:28:48.600847 2023-12-09 08:30:10.126778   \n14      14    -2.381545 2023-12-09 08:30:10.128778 2023-12-09 08:31:32.641031   \n15      15    -4.986612 2023-12-09 08:31:32.642030 2023-12-09 08:32:57.617105   \n16      16    -4.855034 2023-12-09 08:32:57.619106 2023-12-09 08:34:16.971113   \n17      17    -3.629455 2023-12-09 08:34:16.972113 2023-12-09 08:35:43.552796   \n18      18    -4.689032 2023-12-09 08:35:43.553796 2023-12-09 08:37:13.068679   \n19      19    -4.774988 2023-12-09 08:37:13.069678 2023-12-09 08:38:40.342670   \n20      20    -4.599145 2023-12-09 08:38:40.345671 2023-12-09 08:40:07.487845   \n21      21    -3.401742 2023-12-09 08:40:07.489839 2023-12-09 08:41:36.659139   \n22      22    -4.280439 2023-12-09 08:41:36.661139 2023-12-09 08:43:09.229270   \n23      23    -4.379696 2023-12-09 08:43:09.231334 2023-12-09 08:44:38.895128   \n24      24    -3.678909 2023-12-09 08:44:38.897126 2023-12-09 08:46:10.564573   \n25      25    -3.814435 2023-12-09 08:46:10.565574 2023-12-09 08:47:46.174212   \n26      26    -3.349726 2023-12-09 08:47:46.177213 2023-12-09 08:49:15.640449   \n27      27    -3.940458 2023-12-09 08:49:15.641449 2023-12-09 08:50:44.761380   \n28      28    -3.742682 2023-12-09 08:50:44.762380 2023-12-09 08:52:17.164018   \n29      29    -4.024967 2023-12-09 08:52:17.166018 2023-12-09 08:53:45.203667   \n30      30    -4.267322 2023-12-09 08:53:45.205666 2023-12-09 08:55:17.302633   \n31      31    -2.357124 2023-12-09 08:55:17.304633 2023-12-09 08:56:47.867948   \n32      32    -3.237243 2023-12-09 08:56:47.869949 2023-12-09 08:58:18.896638   \n33      33    -3.461568 2023-12-09 08:58:18.897637 2023-12-09 08:59:52.856255   \n34      34    -3.948450 2023-12-09 08:59:52.857260 2023-12-09 09:01:24.799294   \n35      35    -3.557336 2023-12-09 09:01:24.800293 2023-12-09 09:02:56.243295   \n36      36    -4.323387 2023-12-09 09:02:56.246291 2023-12-09 09:04:25.467424   \n37      37    -3.096507 2023-12-09 09:04:25.470423 2023-12-09 09:05:56.879310   \n38      38    -3.612551 2023-12-09 09:05:56.880309 2023-12-09 09:07:28.320047   \n39      39    -3.864762 2023-12-09 09:07:28.321047 2023-12-09 09:08:55.552852   \n40      40    -4.330948 2023-12-09 09:08:55.554851 2023-12-09 09:10:25.636014   \n41      41    -1.576748 2023-12-09 09:10:25.637011 2023-12-09 09:11:56.082286   \n42      42    -4.940520 2023-12-09 09:11:56.083290 2023-12-09 09:13:27.844788   \n43      43    -3.638197 2023-12-09 09:13:27.846747 2023-12-09 09:14:59.574953   \n44      44    -3.714809 2023-12-09 09:14:59.576953 2023-12-09 09:16:29.777147   \n45      45    -3.259949 2023-12-09 09:16:29.779831 2023-12-09 09:18:02.774922   \n46      46    -3.314278 2023-12-09 09:18:02.775922 2023-12-09 09:19:35.916952   \n47      47    -3.376439 2023-12-09 09:19:35.917951 2023-12-09 09:21:06.924020   \n48      48    -3.848852 2023-12-09 09:21:06.926026 2023-12-09 09:22:38.827428   \n49      49    -5.461335 2023-12-09 09:22:38.830426 2023-12-09 09:24:09.322172   \n\n                 duration  params_batcher  params_learning_rate  \\\n0  0 days 00:01:09.481179            True              0.006544   \n1  0 days 00:01:04.154035           False              0.000054   \n2  0 days 00:01:20.978890            True              0.000173   \n3  0 days 00:01:27.915648           False              0.000226   \n4  0 days 00:01:33.908466           False              0.003004   \n5  0 days 00:01:28.767508            True              0.002098   \n6  0 days 00:01:27.877911           False              0.000016   \n7  0 days 00:01:29.315311            True              0.001657   \n8  0 days 00:01:31.768047           False              0.000186   \n9  0 days 00:01:32.506517           False              0.000011   \n10 0 days 00:01:30.427788           False              0.009049   \n11 0 days 00:01:27.698447           False              0.009829   \n12 0 days 00:01:20.898240           False              0.008067   \n13 0 days 00:01:21.525931           False              0.000773   \n14 0 days 00:01:22.512253           False              0.009689   \n15 0 days 00:01:24.975075           False              0.004834   \n16 0 days 00:01:19.352007           False              0.001029   \n17 0 days 00:01:26.580683           False              0.003539   \n18 0 days 00:01:29.514883            True              0.004747   \n19 0 days 00:01:27.272992           False              0.000900   \n20 0 days 00:01:27.142174           False              0.000484   \n21 0 days 00:01:29.169300           False              0.009530   \n22 0 days 00:01:32.568131           False              0.008456   \n23 0 days 00:01:29.663794           False              0.003090   \n24 0 days 00:01:31.667447           False              0.006140   \n25 0 days 00:01:35.608638           False              0.002063   \n26 0 days 00:01:29.463236            True              0.005981   \n27 0 days 00:01:29.119931           False              0.009656   \n28 0 days 00:01:32.401638           False              0.003981   \n29 0 days 00:01:28.037649            True              0.005961   \n30 0 days 00:01:32.096967           False              0.001533   \n31 0 days 00:01:30.563315           False              0.009388   \n32 0 days 00:01:31.026689           False              0.006048   \n33 0 days 00:01:33.958618           False              0.007067   \n34 0 days 00:01:31.942034           False              0.003322   \n35 0 days 00:01:31.443002           False              0.004934   \n36 0 days 00:01:29.221133            True              0.002535   \n37 0 days 00:01:31.408887           False              0.009703   \n38 0 days 00:01:31.439738           False              0.003915   \n39 0 days 00:01:27.231805            True              0.006716   \n40 0 days 00:01:30.081163           False              0.002274   \n41 0 days 00:01:30.445275           False              0.009958   \n42 0 days 00:01:31.761498           False              0.007637   \n43 0 days 00:01:31.728206           False              0.004352   \n44 0 days 00:01:30.200194           False              0.007628   \n45 0 days 00:01:32.995091           False              0.009918   \n46 0 days 00:01:33.141030           False              0.002776   \n47 0 days 00:01:31.006069           False              0.005060   \n48 0 days 00:01:31.901402           False              0.003877   \n49 0 days 00:01:30.491746            True              0.007126   \n\n    params_n_dim_0  params_n_dim_1  params_n_dim_2  params_n_dim_3  \\\n0            421.0            91.0             NaN             NaN   \n1            342.0           253.0             NaN             NaN   \n2            378.0           388.0           368.0             NaN   \n3            505.0           177.0             NaN             NaN   \n4            295.0           296.0           157.0             NaN   \n5              NaN             NaN             NaN             NaN   \n6             79.0           309.0           186.0             NaN   \n7             19.0           226.0           496.0             NaN   \n8            135.0            48.0             NaN             NaN   \n9            296.0           502.0           253.0             NaN   \n10           207.0           411.0            12.0           505.0   \n11           203.0           402.0            11.0           496.0   \n12           187.0           434.0             1.0           511.0   \n13           195.0           489.0            27.0           509.0   \n14           203.0           407.0            85.0            18.0   \n15             NaN             NaN             NaN             NaN   \n16           131.0             NaN             NaN             NaN   \n17           239.0           348.0            94.0           346.0   \n18           134.0             NaN             NaN             NaN   \n19            24.0           452.0             9.0           354.0   \n20           271.0           440.0           335.0           432.0   \n21           182.0           361.0            14.0           511.0   \n22           239.0           444.0            99.0           507.0   \n23            81.0           327.0             4.0             NaN   \n24           166.0           412.0            83.0           124.0   \n25           238.0           508.0           154.0             NaN   \n26           340.0           375.0            60.0           388.0   \n27            93.0             NaN             NaN             NaN   \n28           212.0           453.0           138.0           253.0   \n29           142.0           299.0             NaN             NaN   \n30           283.0           178.0           215.0             NaN   \n31           209.0           394.0            55.0            20.0   \n32           168.0           401.0            55.0           223.0   \n33           212.0           472.0            50.0           448.0   \n34           315.0           418.0             5.0             NaN   \n35           410.0           359.0           130.0            17.0   \n36           248.0           427.0            48.0             NaN   \n37           111.0           340.0           114.0           433.0   \n38           496.0           261.0            47.0             NaN   \n39           157.0           388.0             2.0           167.0   \n40            60.0           473.0             NaN             NaN   \n41           211.0           394.0            76.0             1.0   \n42           226.0           376.0           327.0            69.0   \n43           263.0           126.0            67.0             NaN   \n44           196.0           263.0            39.0             2.0   \n45           182.0           324.0           454.0            66.0   \n46           218.0           470.0           184.0           461.0   \n47           304.0           384.0            32.0             NaN   \n48           327.0            10.0            72.0           469.0   \n49             NaN             NaN             NaN             NaN   \n\n    params_n_layers     state  \n0                 3  COMPLETE  \n1                 3  COMPLETE  \n2                 4  COMPLETE  \n3                 3  COMPLETE  \n4                 4  COMPLETE  \n5                 1  COMPLETE  \n6                 4  COMPLETE  \n7                 4  COMPLETE  \n8                 3  COMPLETE  \n9                 4  COMPLETE  \n10                5  COMPLETE  \n11                5  COMPLETE  \n12                5  COMPLETE  \n13                5  COMPLETE  \n14                5  COMPLETE  \n15                1  COMPLETE  \n16                2  COMPLETE  \n17                5  COMPLETE  \n18                2  COMPLETE  \n19                5  COMPLETE  \n20                5  COMPLETE  \n21                5  COMPLETE  \n22                5  COMPLETE  \n23                4  COMPLETE  \n24                5  COMPLETE  \n25                4  COMPLETE  \n26                5  COMPLETE  \n27                2  COMPLETE  \n28                5  COMPLETE  \n29                3  COMPLETE  \n30                4  COMPLETE  \n31                5  COMPLETE  \n32                5  COMPLETE  \n33                5  COMPLETE  \n34                4  COMPLETE  \n35                5  COMPLETE  \n36                4  COMPLETE  \n37                5  COMPLETE  \n38                4  COMPLETE  \n39                5  COMPLETE  \n40                3  COMPLETE  \n41                5  COMPLETE  \n42                5  COMPLETE  \n43                4  COMPLETE  \n44                5  COMPLETE  \n45                5  COMPLETE  \n46                5  COMPLETE  \n47                4  COMPLETE  \n48                5  COMPLETE  \n49                1  COMPLETE  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number</th>\n      <th>value</th>\n      <th>datetime_start</th>\n      <th>datetime_complete</th>\n      <th>duration</th>\n      <th>params_batcher</th>\n      <th>params_learning_rate</th>\n      <th>params_n_dim_0</th>\n      <th>params_n_dim_1</th>\n      <th>params_n_dim_2</th>\n      <th>params_n_dim_3</th>\n      <th>params_n_layers</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-3.898855</td>\n      <td>2023-12-09 08:10:22.880619</td>\n      <td>2023-12-09 08:11:32.361798</td>\n      <td>0 days 00:01:09.481179</td>\n      <td>True</td>\n      <td>0.006544</td>\n      <td>421.0</td>\n      <td>91.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>-994.830954</td>\n      <td>2023-12-09 08:11:32.363805</td>\n      <td>2023-12-09 08:12:36.517840</td>\n      <td>0 days 00:01:04.154035</td>\n      <td>False</td>\n      <td>0.000054</td>\n      <td>342.0</td>\n      <td>253.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-5.316375</td>\n      <td>2023-12-09 08:12:36.519843</td>\n      <td>2023-12-09 08:13:57.498733</td>\n      <td>0 days 00:01:20.978890</td>\n      <td>True</td>\n      <td>0.000173</td>\n      <td>378.0</td>\n      <td>388.0</td>\n      <td>368.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>-4.948885</td>\n      <td>2023-12-09 08:13:57.501733</td>\n      <td>2023-12-09 08:15:25.417381</td>\n      <td>0 days 00:01:27.915648</td>\n      <td>False</td>\n      <td>0.000226</td>\n      <td>505.0</td>\n      <td>177.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>-3.602496</td>\n      <td>2023-12-09 08:15:25.419382</td>\n      <td>2023-12-09 08:16:59.327848</td>\n      <td>0 days 00:01:33.908466</td>\n      <td>False</td>\n      <td>0.003004</td>\n      <td>295.0</td>\n      <td>296.0</td>\n      <td>157.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>-224.828629</td>\n      <td>2023-12-09 08:16:59.328859</td>\n      <td>2023-12-09 08:18:28.096367</td>\n      <td>0 days 00:01:28.767508</td>\n      <td>True</td>\n      <td>0.002098</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>-4685.181909</td>\n      <td>2023-12-09 08:18:28.097373</td>\n      <td>2023-12-09 08:19:55.975284</td>\n      <td>0 days 00:01:27.877911</td>\n      <td>False</td>\n      <td>0.000016</td>\n      <td>79.0</td>\n      <td>309.0</td>\n      <td>186.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>-4.801416</td>\n      <td>2023-12-09 08:19:55.977283</td>\n      <td>2023-12-09 08:21:25.292594</td>\n      <td>0 days 00:01:29.315311</td>\n      <td>True</td>\n      <td>0.001657</td>\n      <td>19.0</td>\n      <td>226.0</td>\n      <td>496.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>-19.340666</td>\n      <td>2023-12-09 08:21:25.294594</td>\n      <td>2023-12-09 08:22:57.062641</td>\n      <td>0 days 00:01:31.768047</td>\n      <td>False</td>\n      <td>0.000186</td>\n      <td>135.0</td>\n      <td>48.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>-4561.963113</td>\n      <td>2023-12-09 08:22:57.064599</td>\n      <td>2023-12-09 08:24:29.571116</td>\n      <td>0 days 00:01:32.506517</td>\n      <td>False</td>\n      <td>0.000011</td>\n      <td>296.0</td>\n      <td>502.0</td>\n      <td>253.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>-0.021934</td>\n      <td>2023-12-09 08:24:29.572116</td>\n      <td>2023-12-09 08:25:59.999904</td>\n      <td>0 days 00:01:30.427788</td>\n      <td>False</td>\n      <td>0.009049</td>\n      <td>207.0</td>\n      <td>411.0</td>\n      <td>12.0</td>\n      <td>505.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>-0.052664</td>\n      <td>2023-12-09 08:26:00.001160</td>\n      <td>2023-12-09 08:27:27.699607</td>\n      <td>0 days 00:01:27.698447</td>\n      <td>False</td>\n      <td>0.009829</td>\n      <td>203.0</td>\n      <td>402.0</td>\n      <td>11.0</td>\n      <td>496.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>-0.019113</td>\n      <td>2023-12-09 08:27:27.701608</td>\n      <td>2023-12-09 08:28:48.599848</td>\n      <td>0 days 00:01:20.898240</td>\n      <td>False</td>\n      <td>0.008067</td>\n      <td>187.0</td>\n      <td>434.0</td>\n      <td>1.0</td>\n      <td>511.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>-4.502964</td>\n      <td>2023-12-09 08:28:48.600847</td>\n      <td>2023-12-09 08:30:10.126778</td>\n      <td>0 days 00:01:21.525931</td>\n      <td>False</td>\n      <td>0.000773</td>\n      <td>195.0</td>\n      <td>489.0</td>\n      <td>27.0</td>\n      <td>509.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>-2.381545</td>\n      <td>2023-12-09 08:30:10.128778</td>\n      <td>2023-12-09 08:31:32.641031</td>\n      <td>0 days 00:01:22.512253</td>\n      <td>False</td>\n      <td>0.009689</td>\n      <td>203.0</td>\n      <td>407.0</td>\n      <td>85.0</td>\n      <td>18.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>-4.986612</td>\n      <td>2023-12-09 08:31:32.642030</td>\n      <td>2023-12-09 08:32:57.617105</td>\n      <td>0 days 00:01:24.975075</td>\n      <td>False</td>\n      <td>0.004834</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>-4.855034</td>\n      <td>2023-12-09 08:32:57.619106</td>\n      <td>2023-12-09 08:34:16.971113</td>\n      <td>0 days 00:01:19.352007</td>\n      <td>False</td>\n      <td>0.001029</td>\n      <td>131.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>-3.629455</td>\n      <td>2023-12-09 08:34:16.972113</td>\n      <td>2023-12-09 08:35:43.552796</td>\n      <td>0 days 00:01:26.580683</td>\n      <td>False</td>\n      <td>0.003539</td>\n      <td>239.0</td>\n      <td>348.0</td>\n      <td>94.0</td>\n      <td>346.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>-4.689032</td>\n      <td>2023-12-09 08:35:43.553796</td>\n      <td>2023-12-09 08:37:13.068679</td>\n      <td>0 days 00:01:29.514883</td>\n      <td>True</td>\n      <td>0.004747</td>\n      <td>134.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>-4.774988</td>\n      <td>2023-12-09 08:37:13.069678</td>\n      <td>2023-12-09 08:38:40.342670</td>\n      <td>0 days 00:01:27.272992</td>\n      <td>False</td>\n      <td>0.000900</td>\n      <td>24.0</td>\n      <td>452.0</td>\n      <td>9.0</td>\n      <td>354.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>-4.599145</td>\n      <td>2023-12-09 08:38:40.345671</td>\n      <td>2023-12-09 08:40:07.487845</td>\n      <td>0 days 00:01:27.142174</td>\n      <td>False</td>\n      <td>0.000484</td>\n      <td>271.0</td>\n      <td>440.0</td>\n      <td>335.0</td>\n      <td>432.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>-3.401742</td>\n      <td>2023-12-09 08:40:07.489839</td>\n      <td>2023-12-09 08:41:36.659139</td>\n      <td>0 days 00:01:29.169300</td>\n      <td>False</td>\n      <td>0.009530</td>\n      <td>182.0</td>\n      <td>361.0</td>\n      <td>14.0</td>\n      <td>511.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>-4.280439</td>\n      <td>2023-12-09 08:41:36.661139</td>\n      <td>2023-12-09 08:43:09.229270</td>\n      <td>0 days 00:01:32.568131</td>\n      <td>False</td>\n      <td>0.008456</td>\n      <td>239.0</td>\n      <td>444.0</td>\n      <td>99.0</td>\n      <td>507.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>-4.379696</td>\n      <td>2023-12-09 08:43:09.231334</td>\n      <td>2023-12-09 08:44:38.895128</td>\n      <td>0 days 00:01:29.663794</td>\n      <td>False</td>\n      <td>0.003090</td>\n      <td>81.0</td>\n      <td>327.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>-3.678909</td>\n      <td>2023-12-09 08:44:38.897126</td>\n      <td>2023-12-09 08:46:10.564573</td>\n      <td>0 days 00:01:31.667447</td>\n      <td>False</td>\n      <td>0.006140</td>\n      <td>166.0</td>\n      <td>412.0</td>\n      <td>83.0</td>\n      <td>124.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>-3.814435</td>\n      <td>2023-12-09 08:46:10.565574</td>\n      <td>2023-12-09 08:47:46.174212</td>\n      <td>0 days 00:01:35.608638</td>\n      <td>False</td>\n      <td>0.002063</td>\n      <td>238.0</td>\n      <td>508.0</td>\n      <td>154.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>-3.349726</td>\n      <td>2023-12-09 08:47:46.177213</td>\n      <td>2023-12-09 08:49:15.640449</td>\n      <td>0 days 00:01:29.463236</td>\n      <td>True</td>\n      <td>0.005981</td>\n      <td>340.0</td>\n      <td>375.0</td>\n      <td>60.0</td>\n      <td>388.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>-3.940458</td>\n      <td>2023-12-09 08:49:15.641449</td>\n      <td>2023-12-09 08:50:44.761380</td>\n      <td>0 days 00:01:29.119931</td>\n      <td>False</td>\n      <td>0.009656</td>\n      <td>93.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>-3.742682</td>\n      <td>2023-12-09 08:50:44.762380</td>\n      <td>2023-12-09 08:52:17.164018</td>\n      <td>0 days 00:01:32.401638</td>\n      <td>False</td>\n      <td>0.003981</td>\n      <td>212.0</td>\n      <td>453.0</td>\n      <td>138.0</td>\n      <td>253.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>-4.024967</td>\n      <td>2023-12-09 08:52:17.166018</td>\n      <td>2023-12-09 08:53:45.203667</td>\n      <td>0 days 00:01:28.037649</td>\n      <td>True</td>\n      <td>0.005961</td>\n      <td>142.0</td>\n      <td>299.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>30</td>\n      <td>-4.267322</td>\n      <td>2023-12-09 08:53:45.205666</td>\n      <td>2023-12-09 08:55:17.302633</td>\n      <td>0 days 00:01:32.096967</td>\n      <td>False</td>\n      <td>0.001533</td>\n      <td>283.0</td>\n      <td>178.0</td>\n      <td>215.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>31</td>\n      <td>-2.357124</td>\n      <td>2023-12-09 08:55:17.304633</td>\n      <td>2023-12-09 08:56:47.867948</td>\n      <td>0 days 00:01:30.563315</td>\n      <td>False</td>\n      <td>0.009388</td>\n      <td>209.0</td>\n      <td>394.0</td>\n      <td>55.0</td>\n      <td>20.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>-3.237243</td>\n      <td>2023-12-09 08:56:47.869949</td>\n      <td>2023-12-09 08:58:18.896638</td>\n      <td>0 days 00:01:31.026689</td>\n      <td>False</td>\n      <td>0.006048</td>\n      <td>168.0</td>\n      <td>401.0</td>\n      <td>55.0</td>\n      <td>223.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>33</td>\n      <td>-3.461568</td>\n      <td>2023-12-09 08:58:18.897637</td>\n      <td>2023-12-09 08:59:52.856255</td>\n      <td>0 days 00:01:33.958618</td>\n      <td>False</td>\n      <td>0.007067</td>\n      <td>212.0</td>\n      <td>472.0</td>\n      <td>50.0</td>\n      <td>448.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>34</td>\n      <td>-3.948450</td>\n      <td>2023-12-09 08:59:52.857260</td>\n      <td>2023-12-09 09:01:24.799294</td>\n      <td>0 days 00:01:31.942034</td>\n      <td>False</td>\n      <td>0.003322</td>\n      <td>315.0</td>\n      <td>418.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>35</td>\n      <td>-3.557336</td>\n      <td>2023-12-09 09:01:24.800293</td>\n      <td>2023-12-09 09:02:56.243295</td>\n      <td>0 days 00:01:31.443002</td>\n      <td>False</td>\n      <td>0.004934</td>\n      <td>410.0</td>\n      <td>359.0</td>\n      <td>130.0</td>\n      <td>17.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>36</td>\n      <td>-4.323387</td>\n      <td>2023-12-09 09:02:56.246291</td>\n      <td>2023-12-09 09:04:25.467424</td>\n      <td>0 days 00:01:29.221133</td>\n      <td>True</td>\n      <td>0.002535</td>\n      <td>248.0</td>\n      <td>427.0</td>\n      <td>48.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>37</td>\n      <td>-3.096507</td>\n      <td>2023-12-09 09:04:25.470423</td>\n      <td>2023-12-09 09:05:56.879310</td>\n      <td>0 days 00:01:31.408887</td>\n      <td>False</td>\n      <td>0.009703</td>\n      <td>111.0</td>\n      <td>340.0</td>\n      <td>114.0</td>\n      <td>433.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>38</td>\n      <td>-3.612551</td>\n      <td>2023-12-09 09:05:56.880309</td>\n      <td>2023-12-09 09:07:28.320047</td>\n      <td>0 days 00:01:31.439738</td>\n      <td>False</td>\n      <td>0.003915</td>\n      <td>496.0</td>\n      <td>261.0</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>39</td>\n      <td>-3.864762</td>\n      <td>2023-12-09 09:07:28.321047</td>\n      <td>2023-12-09 09:08:55.552852</td>\n      <td>0 days 00:01:27.231805</td>\n      <td>True</td>\n      <td>0.006716</td>\n      <td>157.0</td>\n      <td>388.0</td>\n      <td>2.0</td>\n      <td>167.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>40</td>\n      <td>-4.330948</td>\n      <td>2023-12-09 09:08:55.554851</td>\n      <td>2023-12-09 09:10:25.636014</td>\n      <td>0 days 00:01:30.081163</td>\n      <td>False</td>\n      <td>0.002274</td>\n      <td>60.0</td>\n      <td>473.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>-1.576748</td>\n      <td>2023-12-09 09:10:25.637011</td>\n      <td>2023-12-09 09:11:56.082286</td>\n      <td>0 days 00:01:30.445275</td>\n      <td>False</td>\n      <td>0.009958</td>\n      <td>211.0</td>\n      <td>394.0</td>\n      <td>76.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>42</td>\n      <td>-4.940520</td>\n      <td>2023-12-09 09:11:56.083290</td>\n      <td>2023-12-09 09:13:27.844788</td>\n      <td>0 days 00:01:31.761498</td>\n      <td>False</td>\n      <td>0.007637</td>\n      <td>226.0</td>\n      <td>376.0</td>\n      <td>327.0</td>\n      <td>69.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>43</td>\n      <td>-3.638197</td>\n      <td>2023-12-09 09:13:27.846747</td>\n      <td>2023-12-09 09:14:59.574953</td>\n      <td>0 days 00:01:31.728206</td>\n      <td>False</td>\n      <td>0.004352</td>\n      <td>263.0</td>\n      <td>126.0</td>\n      <td>67.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>44</td>\n      <td>-3.714809</td>\n      <td>2023-12-09 09:14:59.576953</td>\n      <td>2023-12-09 09:16:29.777147</td>\n      <td>0 days 00:01:30.200194</td>\n      <td>False</td>\n      <td>0.007628</td>\n      <td>196.0</td>\n      <td>263.0</td>\n      <td>39.0</td>\n      <td>2.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>-3.259949</td>\n      <td>2023-12-09 09:16:29.779831</td>\n      <td>2023-12-09 09:18:02.774922</td>\n      <td>0 days 00:01:32.995091</td>\n      <td>False</td>\n      <td>0.009918</td>\n      <td>182.0</td>\n      <td>324.0</td>\n      <td>454.0</td>\n      <td>66.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>46</td>\n      <td>-3.314278</td>\n      <td>2023-12-09 09:18:02.775922</td>\n      <td>2023-12-09 09:19:35.916952</td>\n      <td>0 days 00:01:33.141030</td>\n      <td>False</td>\n      <td>0.002776</td>\n      <td>218.0</td>\n      <td>470.0</td>\n      <td>184.0</td>\n      <td>461.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>47</td>\n      <td>-3.376439</td>\n      <td>2023-12-09 09:19:35.917951</td>\n      <td>2023-12-09 09:21:06.924020</td>\n      <td>0 days 00:01:31.006069</td>\n      <td>False</td>\n      <td>0.005060</td>\n      <td>304.0</td>\n      <td>384.0</td>\n      <td>32.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>48</td>\n      <td>-3.848852</td>\n      <td>2023-12-09 09:21:06.926026</td>\n      <td>2023-12-09 09:22:38.827428</td>\n      <td>0 days 00:01:31.901402</td>\n      <td>False</td>\n      <td>0.003877</td>\n      <td>327.0</td>\n      <td>10.0</td>\n      <td>72.0</td>\n      <td>469.0</td>\n      <td>5</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>49</td>\n      <td>-5.461335</td>\n      <td>2023-12-09 09:22:38.830426</td>\n      <td>2023-12-09 09:24:09.322172</td>\n      <td>0 days 00:01:30.491746</td>\n      <td>True</td>\n      <td>0.007126</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>COMPLETE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_default = study_default.trials_dataframe()\n",
    "df_default.to_csv(\"study_label-default_baseline_normalized.csv\")\n",
    "display(df_default)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
